{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfc01410",
   "metadata": {},
   "source": [
    "\n",
    "#  üìä Data Ingestion and Quality Checks\n",
    "# \n",
    "#  Notebook Overview\n",
    "# This notebook performs the initial data ingestion and quality assessment for the Stock Price Indicator project.\n",
    "# \n",
    "#  Objectives:\n",
    "# 1. **API Testing**: Verify yfinance connectivity with single ticker\n",
    "# 2. **Data Collection**: Download historical data for multiple tickers\n",
    "# 3. **Quality Assessment**: Check for missing values, anomalies, and data consistency\n",
    "# 4. **Initial Exploration**: Basic statistics and time series visualization\n",
    "# 5. **Data Storage**: Save processed data for subsequent analysis\n",
    "# \n",
    "#  Target Stocks:\n",
    "# - **AAPL** (Apple Inc.)\n",
    "# - **GOOGL** (Alphabet Inc.)\n",
    "# - **MSFT** (Microsoft Corporation)\n",
    "# - **TSLA** (Tesla Inc.)\n",
    "# - **AMZN** (Amazon.com Inc.)\n",
    "# \n",
    "# ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b31db0c",
   "metadata": {},
   "source": [
    "\n",
    "#  1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4319fb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yfinance as yf\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "plt.style.use('seaborn-v0_8')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")\n",
    "print(\n",
    "    f\"üìÖ Notebook execution date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# %%\n",
    "# Project configuration\n",
    "PROJECT_CONFIG = {\n",
    "    'tickers': ['AAPL', 'GOOGL', 'MSFT', 'TSLA', 'AMZN'],\n",
    "    'start_date': '2015-01-01',\n",
    "    'end_date': datetime.now().strftime('%Y-%m-%d'),\n",
    "    'data_interval': '1d',  # Daily data\n",
    "    'target_column': 'Adj Close',\n",
    "    'prediction_horizons': [1, 7, 14, 28]  # Days ahead to predict\n",
    "}\n",
    "\n",
    "# Directory structure\n",
    "BASE_DIR = Path('../')\n",
    "DATA_DIR = BASE_DIR / 'data'\n",
    "RAW_DATA_DIR = DATA_DIR / 'raw'\n",
    "PROCESSED_DATA_DIR = DATA_DIR / 'processed'\n",
    "FIGURES_DIR = BASE_DIR / 'experiments' / 'figures'\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for directory in [RAW_DATA_DIR, PROCESSED_DATA_DIR, FIGURES_DIR]:\n",
    "    directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"üìÅ Directory structure initialized\")\n",
    "print(f\"üìä Target tickers: {PROJECT_CONFIG['tickers']}\")\n",
    "print(\n",
    "    f\"üìÖ Date range: {PROJECT_CONFIG['start_date']} to {PROJECT_CONFIG['end_date']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142f5889",
   "metadata": {},
   "source": [
    "#  2. API Connectivity Test\n",
    "# \n",
    "# First, let's test the yfinance API with a single ticker to ensure connectivity and understand the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee433981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test API connectivity with AAPL\n",
    "print(\"üîç Testing yfinance API connectivity...\")\n",
    "\n",
    "try:\n",
    "    # Download sample data for AAPL\n",
    "    test_ticker = 'AAPL'\n",
    "    test_data = yf.download(\n",
    "        test_ticker,\n",
    "        start='2024-01-01',\n",
    "        end='2024-12-31',\n",
    "        progress=False\n",
    "    )\n",
    "\n",
    "    print(f\"‚úÖ Successfully connected to yfinance API\")\n",
    "    print(f\"üìà Sample data shape for {test_ticker}: {test_data.shape}\")\n",
    "    print(f\"üìä Available columns: {list(test_data.columns)}\")\n",
    "    print(f\"üìÖ Date range: {test_data.index.min()} to {test_data.index.max()}\")\n",
    "\n",
    "    # Display sample data\n",
    "    print(\"\\nüìã Sample data (first 5 rows):\")\n",
    "    print(test_data.head())\n",
    "\n",
    "    # Check for missing values\n",
    "    missing_values = test_data.isnull().sum()\n",
    "    print(f\"\\nüîç Missing values in test data:\")\n",
    "    for col, missing in missing_values.items():\n",
    "        if missing > 0:\n",
    "            print(f\"  {col}: {missing} ({missing/len(test_data)*100:.2f}%)\")\n",
    "        else:\n",
    "            print(f\"  {col}: ‚úÖ No missing values\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error connecting to yfinance API: {str(e)}\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffc751d",
   "metadata": {},
   "source": [
    "#  3. Multi-Ticker Data Download\n",
    "# \n",
    "# Now let's download historical data for all target tickers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd87e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_stock_data(tickers, start_date, end_date, save_individual=True):\n",
    "    \"\"\"\n",
    "    Download stock data for multiple tickers and save to CSV files.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    tickers : list\n",
    "        List of stock ticker symbols\n",
    "    start_date : str\n",
    "        Start date in YYYY-MM-DD format\n",
    "    end_date : str\n",
    "        End date in YYYY-MM-DD format\n",
    "    save_individual : bool\n",
    "        Whether to save individual CSV files for each ticker\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary containing DataFrames for each ticker\n",
    "    \"\"\"\n",
    "    stock_data = {}\n",
    "    download_summary = []\n",
    "\n",
    "    print(f\"üì• Downloading data for {len(tickers)} tickers...\")\n",
    "    print(f\"üìÖ Date range: {start_date} to {end_date}\\n\")\n",
    "\n",
    "    for ticker in tickers:\n",
    "        try:\n",
    "            print(f\"‚è≥ Downloading {ticker}...\")\n",
    "\n",
    "            # Download data\n",
    "            data = yf.download(\n",
    "                ticker,\n",
    "                start=start_date,\n",
    "                end=end_date,\n",
    "                progress=False\n",
    "            )\n",
    "\n",
    "            if data.empty:\n",
    "                print(f\"‚ö†Ô∏è No data found for {ticker}\")\n",
    "                continue\n",
    "\n",
    "            # Add ticker column for identification\n",
    "            data['Ticker'] = ticker\n",
    "\n",
    "            # Store in dictionary\n",
    "            stock_data[ticker] = data.copy()\n",
    "\n",
    "            # Save individual CSV if requested\n",
    "            if save_individual:\n",
    "                csv_path = RAW_DATA_DIR / f\"{ticker}_raw.csv\"\n",
    "                data.to_csv(csv_path)\n",
    "                print(f\"üíæ Saved {ticker} data to {csv_path}\")\n",
    "\n",
    "            # Collect summary statistics\n",
    "            summary = {\n",
    "                'Ticker': ticker,\n",
    "                'Start_Date': data.index.min(),\n",
    "                'End_Date': data.index.max(),\n",
    "                'Total_Days': len(data),\n",
    "                'Missing_Values': data.isnull().sum().sum(),\n",
    "                'Avg_Volume': data['Volume'].mean(),\n",
    "                'Price_Range': f\"${data['Low'].min():.2f} - ${data['High'].max():.2f}\"\n",
    "            }\n",
    "            download_summary.append(summary)\n",
    "\n",
    "            print(\n",
    "                f\"‚úÖ {ticker}: {len(data)} records from {data.index.min().date()} to {data.index.max().date()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error downloading {ticker}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    # Create summary DataFrame\n",
    "    summary_df = pd.DataFrame(download_summary)\n",
    "\n",
    "    print(f\"\\nüìä Download Summary:\")\n",
    "    print(summary_df.to_string(index=False))\n",
    "\n",
    "    return stock_data, summary_df\n",
    "\n",
    "\n",
    "# Download data for all tickers\n",
    "stock_data, download_summary = download_stock_data(\n",
    "    PROJECT_CONFIG['tickers'],\n",
    "    PROJECT_CONFIG['start_date'],\n",
    "    PROJECT_CONFIG['end_date']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdf23e1",
   "metadata": {},
   "source": [
    "#  4. Data Quality Assessment\n",
    "# \n",
    "# Let's perform comprehensive quality checks on our downloaded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb756624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_data_quality(stock_data):\n",
    "    \"\"\"\n",
    "    Perform comprehensive data quality assessment.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    stock_data : dict\n",
    "        Dictionary containing DataFrames for each ticker\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Quality assessment results\n",
    "    \"\"\"\n",
    "    quality_results = {}\n",
    "\n",
    "    print(\"üîç Performing Data Quality Assessment...\\n\")\n",
    "\n",
    "    for ticker, data in stock_data.items():\n",
    "        print(f\"üìä Analyzing {ticker}...\")\n",
    "\n",
    "        # Basic info\n",
    "        total_records = len(data)\n",
    "        date_range = (data.index.max() - data.index.min()).days\n",
    "\n",
    "        # Missing values analysis\n",
    "        missing_analysis = {}\n",
    "        for col in data.columns:\n",
    "            if col != 'Ticker':\n",
    "                missing_count = data[col].isnull().sum()\n",
    "                missing_pct = (missing_count / total_records) * 100\n",
    "                missing_analysis[col] = {\n",
    "                    'count': missing_count,\n",
    "                    'percentage': missing_pct\n",
    "                }\n",
    "\n",
    "        # Date continuity check\n",
    "        expected_business_days = pd.bdate_range(\n",
    "            start=data.index.min(),\n",
    "            end=data.index.max()\n",
    "        )\n",
    "        actual_days = data.index\n",
    "        missing_dates = set(expected_business_days) - set(actual_days)\n",
    "\n",
    "        # Price anomaly detection\n",
    "        price_cols = ['Open', 'High', 'Low', 'Close', 'Adj Close']\n",
    "        anomalies = {}\n",
    "\n",
    "        for col in price_cols:\n",
    "            if col in data.columns:\n",
    "                # Check for zero or negative prices\n",
    "                zero_negative = (data[col] <= 0).sum()\n",
    "\n",
    "                # Check for extreme price jumps (>20% daily change)\n",
    "                pct_change = data[col].pct_change().abs()\n",
    "                extreme_changes = (pct_change > 0.20).sum()\n",
    "\n",
    "                anomalies[col] = {\n",
    "                    'zero_negative': zero_negative,\n",
    "                    'extreme_changes': extreme_changes\n",
    "                }\n",
    "\n",
    "        # Volume analysis\n",
    "        volume_stats = {\n",
    "            'zero_volume_days': (data['Volume'] == 0).sum() if 'Volume' in data.columns else 0,\n",
    "            'avg_volume': data['Volume'].mean() if 'Volume' in data.columns else 0,\n",
    "            'volume_spikes': (data['Volume'] > data['Volume'].quantile(0.99)).sum() if 'Volume' in data.columns else 0\n",
    "        }\n",
    "\n",
    "        # High-Low consistency check\n",
    "        hl_inconsistent = 0\n",
    "        if all(col in data.columns for col in ['High', 'Low', 'Open', 'Close']):\n",
    "            # Check if High >= Low, High >= Open, High >= Close, etc.\n",
    "            hl_inconsistent = ((data['High'] < data['Low']) |\n",
    "                               (data['High'] < data['Open']) |\n",
    "                               (data['High'] < data['Close']) |\n",
    "                               (data['Low'] > data['Open']) |\n",
    "                               (data['Low'] > data['Close'])).sum()\n",
    "\n",
    "        # Store results\n",
    "        quality_results[ticker] = {\n",
    "            'total_records': total_records,\n",
    "            'date_range_days': date_range,\n",
    "            'missing_dates': len(missing_dates),\n",
    "            'missing_values': missing_analysis,\n",
    "            'price_anomalies': anomalies,\n",
    "            'volume_stats': volume_stats,\n",
    "            'hl_inconsistent': hl_inconsistent\n",
    "        }\n",
    "\n",
    "        # Print summary for this ticker\n",
    "        print(f\"  üìà Total records: {total_records:,}\")\n",
    "        print(f\"  üìÖ Date range: {date_range:,} days\")\n",
    "        print(f\"  üóìÔ∏è Missing business days: {len(missing_dates)}\")\n",
    "        print(f\"  ‚ö†Ô∏è OHLC inconsistencies: {hl_inconsistent}\")\n",
    "        print(f\"  üìä Zero volume days: {volume_stats['zero_volume_days']}\")\n",
    "        print(\"\")\n",
    "\n",
    "    return quality_results\n",
    "\n",
    "\n",
    "# Perform quality assessment\n",
    "quality_results = assess_data_quality(stock_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5895c983",
   "metadata": {},
   "source": [
    "## 5. Missing Values Analysis and Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fb4e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_missing_values(stock_data):\n",
    "    \"\"\"\n",
    "    Detailed analysis of missing values across all tickers.\n",
    "    \"\"\"\n",
    "    print(\"üîç Missing Values Analysis\\n\")\n",
    "\n",
    "    # Create consolidated missing values summary\n",
    "    missing_summary = []\n",
    "\n",
    "    for ticker, data in stock_data.items():\n",
    "        for col in data.columns:\n",
    "            if col != 'Ticker':\n",
    "                missing_count = data[col].isnull().sum()\n",
    "                if missing_count > 0:\n",
    "                    missing_summary.append({\n",
    "                        'Ticker': ticker,\n",
    "                        'Column': col,\n",
    "                        'Missing_Count': missing_count,\n",
    "                        'Missing_Percentage': (missing_count / len(data)) * 100,\n",
    "                        'Total_Records': len(data)\n",
    "                    })\n",
    "\n",
    "    if missing_summary:\n",
    "        missing_df = pd.DataFrame(missing_summary)\n",
    "        print(\"üìã Missing Values Summary:\")\n",
    "        print(missing_df.to_string(index=False))\n",
    "\n",
    "        # Visualization\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "        # Missing values by ticker\n",
    "        ticker_missing = missing_df.groupby('Ticker')['Missing_Count'].sum()\n",
    "        ticker_missing.plot(kind='bar', ax=axes[0], color='coral')\n",
    "        axes[0].set_title('Total Missing Values by Ticker')\n",
    "        axes[0].set_ylabel('Missing Count')\n",
    "        axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "        # Missing values by column\n",
    "        col_missing = missing_df.groupby('Column')['Missing_Count'].sum()\n",
    "        col_missing.plot(kind='bar', ax=axes[1], color='lightblue')\n",
    "        axes[1].set_title('Total Missing Values by Column')\n",
    "        axes[1].set_ylabel('Missing Count')\n",
    "        axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(FIGURES_DIR / 'missing_values_analysis.png',\n",
    "                    dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        print(\"‚úÖ No missing values found in any dataset!\")\n",
    "\n",
    "    return missing_df if missing_summary else None\n",
    "\n",
    "\n",
    "missing_analysis = analyze_missing_values(stock_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef7c3e2",
   "metadata": {},
   "source": [
    "#  6. Basic Time Series Visualization\n",
    "# \n",
    "# Let's create visualizations to understand the time series characteristics of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c7660e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_price_visualizations(stock_data, save_plots=True):\n",
    "    \"\"\"\n",
    "    Create comprehensive price visualizations for all tickers.\n",
    "    \"\"\"\n",
    "    print(\"üìä Creating Price Visualizations...\\n\")\n",
    "\n",
    "    # 1. Adjusted Close Price Comparison\n",
    "    fig, ax = plt.subplots(figsize=(15, 8))\n",
    "\n",
    "    for ticker, data in stock_data.items():\n",
    "        ax.plot(data.index, data['Adj Close'], label=ticker, linewidth=2)\n",
    "\n",
    "    ax.set_title('Adjusted Close Prices Comparison (2015-2024)',\n",
    "                 fontsize=16, fontweight='bold')\n",
    "    ax.set_xlabel('Date', fontsize=12)\n",
    "    ax.set_ylabel('Adjusted Close Price ($)', fontsize=12)\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    if save_plots:\n",
    "        plt.savefig(FIGURES_DIR / 'adj_close_comparison.png',\n",
    "                    dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # 2. Normalized Price Comparison (Base = 100)\n",
    "    fig, ax = plt.subplots(figsize=(15, 8))\n",
    "\n",
    "    for ticker, data in stock_data.items():\n",
    "        normalized_price = (data['Adj Close'] /\n",
    "                            data['Adj Close'].iloc[0]) * 100\n",
    "        ax.plot(data.index, normalized_price, label=ticker, linewidth=2)\n",
    "\n",
    "    ax.set_title('Normalized Adjusted Close Prices (Base = 100 at Start)',\n",
    "                 fontsize=16, fontweight='bold')\n",
    "    ax.set_xlabel('Date', fontsize=12)\n",
    "    ax.set_ylabel('Normalized Price (Base = 100)', fontsize=12)\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.axhline(y=100, color='black', linestyle='--', alpha=0.5)\n",
    "\n",
    "    if save_plots:\n",
    "        plt.savefig(FIGURES_DIR / 'normalized_price_comparison.png',\n",
    "                    dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # 3. Individual ticker subplots with volume\n",
    "    fig, axes = plt.subplots(\n",
    "        len(stock_data), 2, figsize=(20, 4*len(stock_data)))\n",
    "    if len(stock_data) == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "\n",
    "    for i, (ticker, data) in enumerate(stock_data.items()):\n",
    "        # Price plot\n",
    "        axes[i, 0].plot(data.index, data['Adj Close'],\n",
    "                        color='blue', linewidth=1.5)\n",
    "        axes[i, 0].set_title(\n",
    "            f'{ticker} - Adjusted Close Price', fontweight='bold')\n",
    "        axes[i, 0].set_ylabel('Price ($)')\n",
    "        axes[i, 0].grid(True, alpha=0.3)\n",
    "\n",
    "        # Volume plot\n",
    "        axes[i, 1].bar(data.index, data['Volume'],\n",
    "                       alpha=0.7, color='orange', width=1)\n",
    "        axes[i, 1].set_title(f'{ticker} - Trading Volume', fontweight='bold')\n",
    "        axes[i, 1].set_ylabel('Volume')\n",
    "        axes[i, 1].grid(True, alpha=0.3)\n",
    "\n",
    "        # Format x-axis\n",
    "        axes[i, 0].tick_params(axis='x', rotation=45)\n",
    "        axes[i, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save_plots:\n",
    "        plt.savefig(FIGURES_DIR / 'individual_price_volume.png',\n",
    "                    dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # 4. Daily Returns Distribution\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    returns_data = []\n",
    "    for ticker, data in stock_data.items():\n",
    "        daily_returns = data['Adj Close'].pct_change().dropna()\n",
    "        returns_data.append(daily_returns)\n",
    "\n",
    "        # Plot histogram\n",
    "        ax.hist(daily_returns, bins=50, alpha=0.6, label=ticker, density=True)\n",
    "\n",
    "    ax.set_title('Daily Returns Distribution', fontsize=16, fontweight='bold')\n",
    "    ax.set_xlabel('Daily Return', fontsize=12)\n",
    "    ax.set_ylabel('Density', fontsize=12)\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.axvline(x=0, color='black', linestyle='--', alpha=0.5)\n",
    "\n",
    "    if save_plots:\n",
    "        plt.savefig(FIGURES_DIR / 'daily_returns_distribution.png',\n",
    "                    dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "create_price_visualizations(stock_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6e5885",
   "metadata": {},
   "source": [
    "#  7. Statistical Summary and Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7879410e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_statistical_summary(stock_data):\n",
    "    \"\"\"\n",
    "    Compute comprehensive statistical summary for all tickers.\n",
    "    \"\"\"\n",
    "    print(\"üìä Computing Statistical Summary...\\n\")\n",
    "\n",
    "    summary_stats = []\n",
    "    correlation_data = {}\n",
    "\n",
    "    for ticker, data in stock_data.items():\n",
    "        # Basic statistics for Adjusted Close\n",
    "        adj_close = data['Adj Close']\n",
    "        daily_returns = adj_close.pct_change().dropna()\n",
    "\n",
    "        stats = {\n",
    "            'Ticker': ticker,\n",
    "            'Start_Price': adj_close.iloc[0],\n",
    "            'End_Price': adj_close.iloc[-1],\n",
    "            'Min_Price': adj_close.min(),\n",
    "            'Max_Price': adj_close.max(),\n",
    "            'Mean_Price': adj_close.mean(),\n",
    "            'Std_Price': adj_close.std(),\n",
    "            'Total_Return': ((adj_close.iloc[-1] / adj_close.iloc[0]) - 1) * 100,\n",
    "            'Annualized_Return': (((adj_close.iloc[-1] / adj_close.iloc[0]) ** (252 / len(adj_close))) - 1) * 100,\n",
    "            'Daily_Return_Mean': daily_returns.mean() * 100,\n",
    "            'Daily_Return_Std': daily_returns.std() * 100,\n",
    "            'Sharpe_Ratio': (daily_returns.mean() / daily_returns.std()) * np.sqrt(252) if daily_returns.std() > 0 else 0,\n",
    "            'Max_Drawdown': ((adj_close / adj_close.expanding().max() - 1).min()) * 100,\n",
    "            'Avg_Volume': data['Volume'].mean(),\n",
    "            'Volume_Std': data['Volume'].std()\n",
    "        }\n",
    "        summary_stats.append(stats)\n",
    "\n",
    "        # Store returns for correlation analysis\n",
    "        correlation_data[ticker] = daily_returns\n",
    "\n",
    "    # Create summary DataFrame\n",
    "    summary_df = pd.DataFrame(summary_stats)\n",
    "\n",
    "    # Display formatted summary\n",
    "    print(\"üìã Statistical Summary:\")\n",
    "    pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "    print(summary_df.to_string(index=False))\n",
    "\n",
    "    # Correlation analysis\n",
    "    returns_df = pd.DataFrame(correlation_data)\n",
    "    correlation_matrix = returns_df.corr()\n",
    "\n",
    "    print(f\"\\nüîó Daily Returns Correlation Matrix:\")\n",
    "    print(correlation_matrix.round(4))\n",
    "\n",
    "    # Correlation heatmap\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='RdYlBu_r', center=0,\n",
    "                square=True, fmt='.3f', cbar_kws={'label': 'Correlation'})\n",
    "    ax.set_title('Daily Returns Correlation Matrix',\n",
    "                 fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIGURES_DIR / 'correlation_heatmap.png',\n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    return summary_df, correlation_matrix\n",
    "\n",
    "\n",
    "summary_stats, correlation_matrix = compute_statistical_summary(stock_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e86f00",
   "metadata": {},
   "source": [
    "#  8. Anomaly Detection\n",
    "# \n",
    "# Let's identify potential anomalies in the data that might affect our modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3384aa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomalies(stock_data):\n",
    "    \"\"\"\n",
    "    Detect various types of anomalies in stock data.\n",
    "    \"\"\"\n",
    "    print(\"üîç Detecting Data Anomalies...\\n\")\n",
    "\n",
    "    anomaly_report = []\n",
    "\n",
    "    for ticker, data in stock_data.items():\n",
    "        print(f\"üìä Analyzing {ticker} for anomalies...\")\n",
    "\n",
    "        # Calculate daily returns\n",
    "        daily_returns = data['Adj Close'].pct_change()\n",
    "\n",
    "        # 1. Extreme price movements (> 3 standard deviations)\n",
    "        returns_mean = daily_returns.mean()\n",
    "        returns_std = daily_returns.std()\n",
    "        extreme_threshold = 3 * returns_std\n",
    "\n",
    "        extreme_moves = daily_returns[abs(\n",
    "            daily_returns - returns_mean) > extreme_threshold]\n",
    "\n",
    "        # 2. Volume spikes (> 99th percentile)\n",
    "        volume_99th = data['Volume'].quantile(0.99)\n",
    "        volume_spikes = data[data['Volume'] > volume_99th]\n",
    "\n",
    "        # 3. Price gaps (> 5% overnight gap)\n",
    "        price_gaps = []\n",
    "        for i in range(1, len(data)):\n",
    "            prev_close = data['Close'].iloc[i-1]\n",
    "            curr_open = data['Open'].iloc[i]\n",
    "            gap = abs((curr_open - prev_close) / prev_close)\n",
    "            if gap > 0.05:  # 5% gap\n",
    "                price_gaps.append({\n",
    "                    'date': data.index[i],\n",
    "                    'gap_percent': gap * 100,\n",
    "                    'prev_close': prev_close,\n",
    "                    'curr_open': curr_open\n",
    "                })\n",
    "\n",
    "        # 4. Zero volume days\n",
    "        zero_volume_days = data[data['Volume'] == 0]\n",
    "\n",
    "        # 5. OHLC inconsistencies\n",
    "        ohlc_issues = data[\n",
    "            (data['High'] < data['Low']) |\n",
    "            (data['High'] < data['Open']) |\n",
    "            (data['High'] < data['Close']) |\n",
    "            (data['Low'] > data['Open']) |\n",
    "            (data['Low'] > data['Close'])\n",
    "        ]\n",
    "\n",
    "        # Compile report\n",
    "        anomalies = {\n",
    "            'Ticker': ticker,\n",
    "            'Extreme_Returns_Count': len(extreme_moves),\n",
    "            'Max_Single_Day_Gain': daily_returns.max() * 100 if not daily_returns.empty else 0,\n",
    "            'Max_Single_Day_Loss': daily_returns.min() * 100 if not daily_returns.empty else 0,\n",
    "            'Volume_Spikes_Count': len(volume_spikes),\n",
    "            'Price_Gaps_Count': len(price_gaps),\n",
    "            'Zero_Volume_Days': len(zero_volume_days),\n",
    "            'OHLC_Inconsistencies': len(ohlc_issues)\n",
    "        }\n",
    "        anomaly_report.append(anomalies)\n",
    "\n",
    "        # Print details for significant anomalies\n",
    "        if len(extreme_moves) > 0:\n",
    "            print(f\"  ‚ö†Ô∏è {len(extreme_moves)} extreme price movements detected\")\n",
    "            for date, return_val in extreme_moves.head(3).items():\n",
    "                print(f\"    üìÖ {date.date()}: {return_val*100:.2f}% return\")\n",
    "\n",
    "        if len(price_gaps) > 0:\n",
    "            print(f\"  üìà {len(price_gaps)} significant price gaps detected\")\n",
    "            for gap in price_gaps[:3]:\n",
    "                print(\n",
    "                    f\"    üìÖ {gap['date'].date()}: {gap['gap_percent']:.2f}% gap\")\n",
    "\n",
    "        if len(zero_volume_days) > 0:\n",
    "            print(f\"  üìä {len(zero_volume_days)} zero volume days detected\")\n",
    "\n",
    "        print(\"\")\n",
    "\n",
    "    # Create anomaly summary DataFrame\n",
    "    anomaly_df = pd.DataFrame(anomaly_report)\n",
    "    print(\"üìã Anomaly Summary:\")\n",
    "    print(anomaly_df.to_string(index=False))\n",
    "\n",
    "    return anomaly_df\n",
    "\n",
    "\n",
    "anomaly_report = detect_anomalies(stock_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4bb0ff",
   "metadata": {},
   "source": [
    "#  9. Data Export and Preparation for Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427dc795",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_processed_data(stock_data, summary_stats, quality_results):\n",
    "    \"\"\"\n",
    "    Export processed data and analysis results for next steps.\n",
    "    \"\"\"\n",
    "    print(\"üíæ Exporting processed data and analysis results...\\n\")\n",
    "\n",
    "    # 1. Create combined dataset with all tickers\n",
    "    combined_data = []\n",
    "    for ticker, data in stock_data.items():\n",
    "        ticker_data = data.copy()\n",
    "        ticker_data['Ticker'] = ticker\n",
    "        ticker_data['Date'] = ticker_data.index\n",
    "        combined_data.append(ticker_data)\n",
    "\n",
    "    all_stocks_df = pd.concat(combined_data, ignore_index=True)\n",
    "    all_stocks_df = all_stocks_df.set_index('Date')\n",
    "\n",
    "    # Save combined dataset\n",
    "    combined_path = PROCESSED_DATA_DIR / 'all_stocks_combined.csv'\n",
    "    all_stocks_df.to_csv(combined_path)\n",
    "    print(f\"üìä Combined dataset saved to: {combined_path}\")\n",
    "    print(f\"   üìà Shape: {all_stocks_df.shape}\")\n",
    "\n",
    "    # 2. Save individual processed datasets\n",
    "    for ticker, data in stock_data.items():\n",
    "        # Add some basic derived features\n",
    "        processed_data = data.copy()\n",
    "        processed_data['Daily_Return'] = processed_data['Adj Close'].pct_change()\n",
    "        processed_data['Log_Return'] = np.log(\n",
    "            processed_data['Adj Close'] / processed_data['Adj Close'].shift(1))\n",
    "        processed_data['Price_Range'] = processed_data['High'] - \\\n",
    "            processed_data['Low']\n",
    "        processed_data['Volume_MA_20'] = processed_data['Volume'].rolling(\n",
    "            window=20).mean()\n",
    "\n",
    "        ticker_path = PROCESSED_DATA_DIR / f'{ticker}_processed.csv'\n",
    "        processed_data.to_csv(ticker_path)\n",
    "        print(f\"üìä {ticker} processed data saved to: {ticker_path}\")\n",
    "\n",
    "    # 3. Save analysis results\n",
    "    summary_stats.to_csv(PROCESSED_DATA_DIR /\n",
    "                         'statistical_summary.csv', index=False)\n",
    "    print(\n",
    "        f\"üìã Statistical summary saved to: {PROCESSED_DATA_DIR / 'statistical_summary.csv'}\")\n",
    "\n",
    "    # 4. Save metadata\n",
    "    metadata = {\n",
    "        'data_ingestion_date': datetime.now().isoformat(),\n",
    "        'tickers': PROJECT_CONFIG['tickers'],\n",
    "        'date_range': f\"{PROJECT_CONFIG['start_date']} to {PROJECT_CONFIG['end_date']}\",\n",
    "        'total_records_per_ticker': {ticker: len(data) for ticker, data in stock_data.items()},\n",
    "        'quality_check_passed': True,  # Would be False if critical issues found\n",
    "        'next_steps': [\n",
    "            'Exploratory Data Analysis (02_eda_and_viz.ipynb)',\n",
    "            'Feature Engineering (03_feature_engineering.ipynb)',\n",
    "            'Baseline Modeling (04_baseline_models.ipynb)'\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    import json\n",
    "    with open(PROCESSED_DATA_DIR / 'data_metadata.json', 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "\n",
    "    print(f\"üìù Metadata saved to: {PROCESSED_DATA_DIR / 'data_metadata.json'}\")\n",
    "\n",
    "    return all_stocks_df\n",
    "\n",
    "\n",
    "# Export all data and results\n",
    "combined_dataset = export_processed_data(\n",
    "    stock_data, summary_stats, quality_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c6c12b",
   "metadata": {},
   "source": [
    "#  10. Summary and Key Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ac1e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Summary and Key Findings\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üìä DATA INGESTION AND QUALITY ASSESSMENT SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n‚úÖ SUCCESSFULLY COMPLETED:\")\n",
    "print(\n",
    "    f\"   üì• Downloaded data for {len(stock_data)} tickers: {', '.join(stock_data.keys())}\")\n",
    "print(\n",
    "    f\"   üìÖ Date range: {PROJECT_CONFIG['start_date']} to {PROJECT_CONFIG['end_date']}\")\n",
    "print(f\"   üíæ Saved {len(stock_data)*2 + 3} files to data directories\")\n",
    "\n",
    "print(f\"\\nüìä DATA OVERVIEW:\")\n",
    "total_records = sum(len(data) for data in stock_data.values())\n",
    "print(f\"   üìà Total records across all tickers: {total_records:,}\")\n",
    "print(\n",
    "    f\"   üóìÔ∏è Average trading days per ticker: {total_records // len(stock_data):,}\")\n",
    "\n",
    "print(f\"\\nüîç QUALITY ASSESSMENT:\")\n",
    "has_missing = missing_analysis is not None and len(missing_analysis) > 0\n",
    "print(\n",
    "    f\"   ‚úÖ Missing values: {'Found issues - see analysis above' if has_missing else 'No missing values detected'}\")\n",
    "\n",
    "# Count total anomalies across all tickers\n",
    "total_extreme_returns = anomaly_report['Extreme_Returns_Count'].sum()\n",
    "total_price_gaps = anomaly_report['Price_Gaps_Count'].sum()\n",
    "total_zero_volume = anomaly_report['Zero_Volume_Days'].sum()\n",
    "total_ohlc_issues = anomaly_report['OHLC_Inconsistencies'].sum()\n",
    "\n",
    "print(\n",
    "    f\"   ‚ö†Ô∏è Extreme returns (>3œÉ): {total_extreme_returns} across all tickers\")\n",
    "print(f\"   üìà Price gaps (>5%): {total_price_gaps} across all tickers\")\n",
    "print(f\"   üìä Zero volume days: {total_zero_volume} across all tickers\")\n",
    "print(f\"   üîß OHLC inconsistencies: {total_ohlc_issues} across all tickers\")\n",
    "\n",
    "print(f\"\\nüìà PERFORMANCE HIGHLIGHTS:\")\n",
    "# Get top and bottom performers\n",
    "best_performer = summary_stats.loc[summary_stats['Total_Return'].idxmax()]\n",
    "worst_performer = summary_stats.loc[summary_stats['Total_Return'].idxmin()]\n",
    "highest_volatility = summary_stats.loc[summary_stats['Daily_Return_Std'].idxmax(\n",
    ")]\n",
    "best_sharpe = summary_stats.loc[summary_stats['Sharpe_Ratio'].idxmax()]\n",
    "\n",
    "print(\n",
    "    f\"   üèÜ Best performer: {best_performer['Ticker']} ({best_performer['Total_Return']:.1f}% total return)\")\n",
    "print(\n",
    "    f\"   üìâ Worst performer: {worst_performer['Ticker']} ({worst_performer['Total_Return']:.1f}% total return)\")\n",
    "print(\n",
    "    f\"   üìä Most volatile: {highest_volatility['Ticker']} ({highest_volatility['Daily_Return_Std']:.2f}% daily std)\")\n",
    "print(\n",
    "    f\"   ‚ö° Best Sharpe ratio: {best_sharpe['Ticker']} ({best_sharpe['Sharpe_Ratio']:.2f})\")\n",
    "\n",
    "print(f\"\\nüîó CORRELATION INSIGHTS:\")\n",
    "# Find highest and lowest correlations\n",
    "corr_values = []\n",
    "tickers = list(stock_data.keys())\n",
    "for i in range(len(tickers)):\n",
    "    for j in range(i+1, len(tickers)):\n",
    "        corr_val = correlation_matrix.loc[tickers[i], tickers[j]]\n",
    "        corr_values.append((tickers[i], tickers[j], corr_val))\n",
    "\n",
    "corr_values.sort(key=lambda x: x[2], reverse=True)\n",
    "highest_corr = corr_values[0]\n",
    "lowest_corr = corr_values[-1]\n",
    "\n",
    "print(\n",
    "    f\"   üìä Highest correlation: {highest_corr[0]} - {highest_corr[1]} ({highest_corr[2]:.3f})\")\n",
    "print(\n",
    "    f\"   üìä Lowest correlation: {lowest_corr[0]} - {lowest_corr[1]} ({lowest_corr[2]:.3f})\")\n",
    "avg_correlation = correlation_matrix.values[np.triu_indices_from(\n",
    "    correlation_matrix.values, k=1)].mean()\n",
    "print(f\"   üìä Average pairwise correlation: {avg_correlation:.3f}\")\n",
    "\n",
    "print(f\"\\nüíæ OUTPUT FILES CREATED:\")\n",
    "print(\n",
    "    f\"   üìÅ Raw data: {len(stock_data)} individual CSV files in {RAW_DATA_DIR}\")\n",
    "print(\n",
    "    f\"   üìÅ Processed data: {len(stock_data)} individual CSV files in {PROCESSED_DATA_DIR}\")\n",
    "print(\n",
    "    f\"   üìÑ Combined dataset: all_stocks_combined.csv ({combined_dataset.shape[0]:,} rows)\")\n",
    "print(f\"   üìä Statistical summary: statistical_summary.csv\")\n",
    "print(f\"   üìã Metadata: data_metadata.json\")\n",
    "print(\n",
    "    f\"   üìä Visualizations: {len([f for f in FIGURES_DIR.glob('*.png')])} charts in {FIGURES_DIR}\")\n",
    "\n",
    "print(f\"\\nüîç DATA QUALITY VERDICT:\")\n",
    "quality_issues = []\n",
    "if has_missing:\n",
    "    quality_issues.append(\"Missing values detected\")\n",
    "# More than 5 extreme moves per ticker on average\n",
    "if total_extreme_returns > len(stock_data) * 5:\n",
    "    quality_issues.append(\"High number of extreme price movements\")\n",
    "if total_zero_volume > 0:\n",
    "    quality_issues.append(\"Zero volume days found\")\n",
    "if total_ohlc_issues > 0:\n",
    "    quality_issues.append(\"OHLC price inconsistencies found\")\n",
    "\n",
    "if not quality_issues:\n",
    "    print(\"   ‚úÖ EXCELLENT: Data quality is high with no significant issues detected\")\n",
    "    print(\"   ‚úÖ All datasets are ready for feature engineering and modeling\")\n",
    "elif len(quality_issues) <= 2:\n",
    "    print(\"   ‚ö†Ô∏è GOOD: Minor data quality issues identified:\")\n",
    "    for issue in quality_issues:\n",
    "        print(f\"      - {issue}\")\n",
    "    print(\"   ‚úÖ Data is suitable for modeling with standard preprocessing\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è CAUTION: Multiple data quality issues identified:\")\n",
    "    for issue in quality_issues:\n",
    "        print(f\"      - {issue}\")\n",
    "    print(\"   üîß Additional data cleaning may be required before modeling\")\n",
    "\n",
    "print(f\"\\nüéØ RECOMMENDED NEXT STEPS:\")\n",
    "print(f\"   1Ô∏è‚É£ Run Exploratory Data Analysis (02_eda_and_viz.ipynb)\")\n",
    "print(f\"   2Ô∏è‚É£ Implement Feature Engineering (03_feature_engineering.ipynb)\")\n",
    "print(f\"   3Ô∏è‚É£ Develop Baseline Models (04_baseline_models.ipynb)\")\n",
    "print(f\"   4Ô∏è‚É£ Consider additional data sources for external factors\")\n",
    "\n",
    "print(f\"\\nüìù KEY TAKEAWAYS:\")\n",
    "print(\n",
    "    f\"   ‚Ä¢ All {len(PROJECT_CONFIG['tickers'])} target stocks successfully downloaded\")\n",
    "print(\n",
    "    f\"   ‚Ä¢ {total_records:,} total data points spanning ~{(pd.to_datetime(PROJECT_CONFIG['end_date']) - pd.to_datetime(PROJECT_CONFIG['start_date'])).days} days\")\n",
    "print(f\"   ‚Ä¢ Strong foundation established for time series prediction modeling\")\n",
    "print(f\"   ‚Ä¢ Data exhibits typical financial time series characteristics\")\n",
    "print(f\"   ‚Ä¢ Ready to proceed with advanced analysis and feature engineering\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üéâ DATA INGESTION PHASE COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddf0f78",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
