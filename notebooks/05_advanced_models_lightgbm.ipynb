{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Models: LightGBM & XGBoost for Stock Price Prediction\n",
    "## Notebook 05 - Advanced ML Models\n",
    "\n",
    "### This notebook implements and evaluates advanced machine learning models \n",
    "### for multi-horizon stock price prediction, building upon baseline models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be60a70",
   "metadata": {},
   "source": [
    "### SETUP AND IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b91ef468",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-24 19:33:50,710 - WARNING - Failed to load logging config, using basic setup: [Errno 2] No such file or directory: '/Users/hossameldinsalama/Downloads/Data-science/Data science course/Udacity/Data scientist/Data Scientist Capstone/capstone-stock-predictor/capstone_stock_predictor/notebooks/logs/pipeline.log'\n",
      "2025-09-24 19:33:51,652 - WARNING - Failed to load logging config, using basic setup: [Errno 2] No such file or directory: '/Users/hossameldinsalama/Downloads/Data-science/Data science course/Udacity/Data scientist/Data Scientist Capstone/capstone-stock-predictor/capstone_stock_predictor/notebooks/logs/pipeline.log'\n",
      "2025-09-24 19:33:51,660 - WARNING - Failed to load logging config, using basic setup: [Errno 2] No such file or directory: '/Users/hossameldinsalama/Downloads/Data-science/Data science course/Udacity/Data scientist/Data Scientist Capstone/capstone-stock-predictor/capstone_stock_predictor/notebooks/logs/pipeline.log'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All imports successful!\n",
      "📊 LightGBM version: 4.6.0\n",
      "🚀 XGBoost version: 2.1.4\n"
     ]
    }
   ],
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from typing import Dict, List, Tuple, Any\n",
    "import time\n",
    "import joblib\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Machine Learning\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "# Project modules - using existing foundation\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.data import (\n",
    "    load_config, setup_logging, load_raw_data, \n",
    "    clean_data, validate_data_quality\n",
    ")\n",
    "from src.features import (\n",
    "    create_all_features, apply_configured_features,\n",
    "    validate_features, scale_features\n",
    ")\n",
    "from src.models import (\n",
    "    LightGBMPredictor, XGBPredictor, tune_lightgbm, tune_xgboost,\n",
    "    train_models_multi_horizon, evaluate_with_walk_forward,\n",
    "    save_model_records\n",
    ")\n",
    "from src.evaluate import (\n",
    "    walk_forward_validation, compare_models, \n",
    "    statistical_significance_test, create_evaluation_report,\n",
    "    calculate_rmse, calculate_mae, calculate_mape, \n",
    "    calculate_directional_accuracy, calculate_within_tolerance\n",
    ")\n",
    "from src.utils import (\n",
    "    ensure_dir_exists, save_results_to_json, \n",
    "    plot_model_comparison, plot_feature_importance,\n",
    "    timer_decorator, get_project_root\n",
    ")\n",
    "\n",
    "# Configure display and warnings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "plt.style.use('default')\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✅ All imports successful!\")\n",
    "print(f\"📊 LightGBM version: {lgb.__version__}\")\n",
    "print(f\"🚀 XGBoost version: {xgb.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fd49e6",
   "metadata": {},
   "source": [
    "\n",
    "### 1- CONFIGURATION AND SETUP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023ae48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration and setup logging\n",
    "config = load_config()\n",
    "logger = setup_logging()\n",
    "logger.info(\"Starting advanced models notebook\")\n",
    "\n",
    "# Extract key parameters\n",
    "DATA_DIR = Path(config['data']['raw_data_dir'])\n",
    "PROCESSED_DIR = Path(config['data']['processed_data_dir'])\n",
    "MODELS_DIR = Path(config['paths']['models_dir'])\n",
    "FIGURES_DIR = Path(config['paths']['figures_dir'])\n",
    "RESULTS_DIR = Path(config['paths']['results_dir'])\n",
    "\n",
    "# Ensure directories exist\n",
    "for dir_path in [PROCESSED_DIR, MODELS_DIR, FIGURES_DIR, RESULTS_DIR]:\n",
    "    ensure_dir_exists(dir_path)\n",
    "\n",
    "# Model parameters\n",
    "PREDICTION_HORIZONS = config['models']['prediction_horizons']\n",
    "TEST_SIZE = config['validation']['test_size']\n",
    "N_SPLITS = config['validation']['n_splits']\n",
    "RANDOM_STATE = config.get('random_state', 42)\n",
    "\n",
    "print(f\"📁 Data directory: {DATA_DIR}\")\n",
    "print(f\"🎯 Prediction horizons: {PREDICTION_HORIZONS}\")\n",
    "print(f\"✅ Configuration loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91257b85",
   "metadata": {},
   "source": [
    "\n",
    "### 2- DATA LOADING AND PREPARATION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1627319",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_data(ticker: str = 'AAPL') -> pd.DataFrame:\n",
    "    \"\"\"Load and prepare data with features for modeling.\"\"\"\n",
    "    logger.info(f\"Loading and preparing data for {ticker}\")\n",
    "    \n",
    "    # Load raw data\n",
    "    file_path = DATA_DIR / f\"{ticker}.csv\"\n",
    "    if not file_path.exists():\n",
    "        raise FileNotFoundError(f\"Data file not found: {file_path}\")\n",
    "    \n",
    "    df = load_raw_data(file_path)\n",
    "    logger.info(f\"Loaded {len(df)} rows of data for {ticker}\")\n",
    "    \n",
    "    # Clean data\n",
    "    df_clean = clean_data(df)\n",
    "    \n",
    "    # Validate data quality\n",
    "    is_valid = validate_data_quality(df_clean, detailed=True)\n",
    "    if not is_valid:\n",
    "        logger.warning(\"Data quality issues detected\")\n",
    "    \n",
    "    # Create features using configured pipeline\n",
    "    df_features = apply_configured_features(df_clean, config)\n",
    "    logger.info(f\"Created features: {df_features.shape}\")\n",
    "    \n",
    "    # Validate features\n",
    "    validation_report = validate_features(df_features)\n",
    "    logger.info(f\"Feature validation: {validation_report}\")\n",
    "    \n",
    "    return df_features\n",
    "\n",
    "# Load sample data for demonstration\n",
    "print(\"📊 Loading sample data...\")\n",
    "try:\n",
    "    # Use AAPL as primary example\n",
    "    sample_data = load_and_prepare_data('AAPL')\n",
    "    print(f\"✅ Data loaded: {sample_data.shape}\")\n",
    "    print(f\"📅 Date range: {sample_data.index.min()} to {sample_data.index.max()}\")\n",
    "    \n",
    "    # Display basic info\n",
    "    print(\"\\n📋 Feature columns:\")\n",
    "    feature_cols = [col for col in sample_data.columns if not col.startswith('target_')]\n",
    "    target_cols = [col for col in sample_data.columns if col.startswith('target_')]\n",
    "    print(f\"Features: {len(feature_cols)} columns\")\n",
    "    print(f\"Targets: {target_cols}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error loading data: {e}\")\n",
    "    print(f\"❌ Error loading data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed488666",
   "metadata": {},
   "source": [
    "\n",
    "### 3- ADVANCED MODELS IMPLEMENTATION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaf9aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedModelTrainer:\n",
    "    \"\"\"Trainer for advanced ML models with hyperparameter optimization.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict[str, Any], random_state: int = 42):\n",
    "        self.config = config\n",
    "        self.random_state = random_state\n",
    "        self.models = {}\n",
    "        self.results = {}\n",
    "        self.feature_importance = {}\n",
    "        \n",
    "    def prepare_model_data(self, data: pd.DataFrame, horizon: int) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "        \"\"\"Prepare features and targets for specific prediction horizon.\"\"\"\n",
    "        target_col = f'target_{horizon}d'\n",
    "        \n",
    "        if target_col not in data.columns:\n",
    "            raise ValueError(f\"Target column {target_col} not found in data\")\n",
    "        \n",
    "        # Get feature columns (exclude targets and any metadata)\n",
    "        feature_cols = [col for col in data.columns \n",
    "                       if not col.startswith('target_') \n",
    "                       and col not in ['symbol', 'sector', 'industry']]\n",
    "        \n",
    "        X = data[feature_cols].copy()\n",
    "        y = data[target_col].copy()\n",
    "        \n",
    "        # Remove rows with NaN targets\n",
    "        valid_mask = ~y.isna()\n",
    "        X = X.loc[valid_mask]\n",
    "        y = y.loc[valid_mask]\n",
    "        \n",
    "        # Handle remaining NaN in features\n",
    "        X = X.fillna(method='ffill').fillna(method='bfill')\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    @timer_decorator\n",
    "    def train_lightgbm_model(self, data: pd.DataFrame, horizon: int, \n",
    "                           optimize_params: bool = True) -> Dict[str, Any]:\n",
    "        \"\"\"Train LightGBM model for specific horizon.\"\"\"\n",
    "        logger.info(f\"Training LightGBM model for {horizon}-day horizon\")\n",
    "        \n",
    "        X, y = self.prepare_model_data(data, horizon)\n",
    "        \n",
    "        if optimize_params:\n",
    "            # Use hyperparameter tuning from models.py\n",
    "            best_params = tune_lightgbm(X, y, n_trials=50, random_state=self.random_state)\n",
    "            logger.info(f\"Best LightGBM params for {horizon}d: {best_params}\")\n",
    "        else:\n",
    "            # Use default parameters\n",
    "            best_params = self.config['models']['lightgbm']['params']\n",
    "        \n",
    "        # Create and train model\n",
    "        model = LightGBMPredictor(**best_params)\n",
    "        \n",
    "        # Evaluate with walk-forward validation\n",
    "        results = evaluate_with_walk_forward(\n",
    "            model, X, y, \n",
    "            test_size=TEST_SIZE,\n",
    "            n_splits=N_SPLITS\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        model_key = f'lightgbm_{horizon}d'\n",
    "        self.models[model_key] = model\n",
    "        self.results[model_key] = results\n",
    "        \n",
    "        # Extract feature importance\n",
    "        if hasattr(model, 'feature_importance_'):\n",
    "            importance_df = pd.DataFrame({\n",
    "                'feature': X.columns,\n",
    "                'importance': model.feature_importance_\n",
    "            }).sort_values('importance', ascending=False)\n",
    "            self.feature_importance[model_key] = importance_df\n",
    "        \n",
    "        logger.info(f\"✅ LightGBM {horizon}d training completed\")\n",
    "        return results\n",
    "    \n",
    "    @timer_decorator\n",
    "    def train_xgboost_model(self, data: pd.DataFrame, horizon: int, \n",
    "                          optimize_params: bool = True) -> Dict[str, Any]:\n",
    "        \"\"\"Train XGBoost model for specific horizon.\"\"\"\n",
    "        logger.info(f\"Training XGBoost model for {horizon}-day horizon\")\n",
    "        \n",
    "        X, y = self.prepare_model_data(data, horizon)\n",
    "        \n",
    "        if optimize_params:\n",
    "            # Use hyperparameter tuning from models.py\n",
    "            best_params = tune_xgboost(X, y, n_trials=50, random_state=self.random_state)\n",
    "            logger.info(f\"Best XGBoost params for {horizon}d: {best_params}\")\n",
    "        else:\n",
    "            # Use default parameters\n",
    "            best_params = self.config['models']['xgboost']['params']\n",
    "        \n",
    "        # Create and train model\n",
    "        model = XGBPredictor(**best_params)\n",
    "        \n",
    "        # Evaluate with walk-forward validation\n",
    "        results = evaluate_with_walk_forward(\n",
    "            model, X, y,\n",
    "            test_size=TEST_SIZE,\n",
    "            n_splits=N_SPLITS\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        model_key = f'xgboost_{horizon}d'\n",
    "        self.models[model_key] = model\n",
    "        self.results[model_key] = results\n",
    "        \n",
    "        # Extract feature importance\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            importance_df = pd.DataFrame({\n",
    "                'feature': X.columns,\n",
    "                'importance': model.feature_importances_\n",
    "            }).sort_values('importance', ascending=False)\n",
    "            self.feature_importance[model_key] = importance_df\n",
    "        \n",
    "        logger.info(f\"✅ XGBoost {horizon}d training completed\")\n",
    "        return results\n",
    "    \n",
    "    def train_all_models(self, data: pd.DataFrame, optimize: bool = True):\n",
    "        \"\"\"Train all models for all prediction horizons.\"\"\"\n",
    "        logger.info(\"Starting comprehensive model training\")\n",
    "        \n",
    "        for horizon in PREDICTION_HORIZONS:\n",
    "            print(f\"\\n🎯 Training models for {horizon}-day horizon...\")\n",
    "            \n",
    "            try:\n",
    "                # Train LightGBM\n",
    "                lgb_results = self.train_lightgbm_model(data, horizon, optimize)\n",
    "                print(f\"✅ LightGBM {horizon}d - RMSE: {lgb_results.get('test_rmse', 'N/A'):.4f}\")\n",
    "                \n",
    "                # Train XGBoost\n",
    "                xgb_results = self.train_xgboost_model(data, horizon, optimize)\n",
    "                print(f\"✅ XGBoost {horizon}d - RMSE: {xgb_results.get('test_rmse', 'N/A'):.4f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error training models for {horizon}d: {e}\")\n",
    "                print(f\"❌ Error training {horizon}d models: {e}\")\n",
    "        \n",
    "        logger.info(\"Model training completed\")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = AdvancedModelTrainer(config, RANDOM_STATE)\n",
    "print(\"🚀 Advanced model trainer initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c026fd14",
   "metadata": {},
   "source": [
    "\n",
    "### 4- MODEL TRAINING AND OPTIMIZATION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0be213",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🎯 STARTING ADVANCED MODEL TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train models (set optimize=False for faster execution during development)\n",
    "OPTIMIZE_HYPERPARAMS = True  # Set to False for quick testing\n",
    "\n",
    "try:\n",
    "    trainer.train_all_models(sample_data, optimize=OPTIMIZE_HYPERPARAMS)\n",
    "    print(f\"\\n✅ Training completed for {len(trainer.models)} models\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error in model training: {e}\")\n",
    "    print(f\"❌ Training error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054b21a6",
   "metadata": {},
   "source": [
    "\n",
    "### 5- RESULTS ANALYSIS AND COMPARISON\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba94e779",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_model_performance(trainer: AdvancedModelTrainer) -> pd.DataFrame:\n",
    "    \"\"\"Analyze and compare model performance across horizons.\"\"\"\n",
    "    \n",
    "    if not trainer.results:\n",
    "        print(\"❌ No model results available\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Compile results\n",
    "    performance_data = []\n",
    "    \n",
    "    for model_name, results in trainer.results.items():\n",
    "        model_type = model_name.split('_')[0]\n",
    "        horizon = model_name.split('_')[1]\n",
    "        \n",
    "        performance_data.append({\n",
    "            'Model': model_type.upper(),\n",
    "            'Horizon': horizon,\n",
    "            'RMSE': results.get('test_rmse', np.nan),\n",
    "            'MAE': results.get('test_mae', np.nan),\n",
    "            'MAPE': results.get('test_mape', np.nan),\n",
    "            'R2': results.get('test_r2', np.nan),\n",
    "            'Directional_Accuracy': results.get('test_directional_accuracy', np.nan),\n",
    "            'Hit_Rate_5pct': results.get('test_hit_rate', np.nan)\n",
    "        })\n",
    "    \n",
    "    performance_df = pd.DataFrame(performance_data)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\n📊 MODEL PERFORMANCE SUMMARY\")\n",
    "    print(\"-\" * 50)\n",
    "    print(performance_df.to_string(index=False, float_format='%.4f'))\n",
    "    \n",
    "    return performance_df\n",
    "\n",
    "# Analyze performance\n",
    "if trainer.results:\n",
    "    performance_summary = analyze_model_performance(trainer)\n",
    "else:\n",
    "    print(\"⚠️ No results to analyze - models may not have trained successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fecc4dd",
   "metadata": {},
   "source": [
    "### 6- FEATURE IMPORTANCE ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cb4457",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_feature_importance(trainer: AdvancedModelTrainer, top_n: int = 20):\n",
    "    \"\"\"Analyze and visualize feature importance across models.\"\"\"\n",
    "    \n",
    "    if not trainer.feature_importance:\n",
    "        print(\"❌ No feature importance data available\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n🔍 TOP {top_n} FEATURE IMPORTANCE ANALYSIS\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    plot_idx = 0\n",
    "    \n",
    "    for model_name, importance_df in trainer.feature_importance.items():\n",
    "        if plot_idx >= len(axes):\n",
    "            break\n",
    "            \n",
    "        # Get top features\n",
    "        top_features = importance_df.head(top_n)\n",
    "        \n",
    "        # Plot\n",
    "        ax = axes[plot_idx]\n",
    "        bars = ax.barh(range(len(top_features)), top_features['importance'])\n",
    "        ax.set_yticks(range(len(top_features)))\n",
    "        ax.set_yticklabels(top_features['feature'])\n",
    "        ax.set_xlabel('Importance')\n",
    "        ax.set_title(f'{model_name.upper()} - Top {top_n} Features')\n",
    "        ax.invert_yaxis()\n",
    "        \n",
    "        # Color gradient\n",
    "        colors = plt.cm.viridis(np.linspace(0, 1, len(top_features)))\n",
    "        for bar, color in zip(bars, colors):\n",
    "            bar.set_color(color)\n",
    "        \n",
    "        plot_idx += 1\n",
    "    \n",
    "    # Remove empty subplots\n",
    "    for idx in range(plot_idx, len(axes)):\n",
    "        fig.delaxes(axes[idx])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIGURES_DIR / 'feature_importance_advanced_models.png', \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print top features for each model\n",
    "    for model_name, importance_df in trainer.feature_importance.items():\n",
    "        print(f\"\\n{model_name.upper()} - Top 10 Features:\")\n",
    "        print(importance_df.head(10)[['feature', 'importance']].to_string(index=False))\n",
    "\n",
    "# Analyze feature importance\n",
    "if trainer.feature_importance:\n",
    "    analyze_feature_importance(trainer, top_n=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ebd473",
   "metadata": {},
   "source": [
    "### 7- MODEL COMPARISON AND STATISTICAL TESTS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce9528e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_with_baseline_models(advanced_results: Dict[str, Any]) -> pd.DataFrame:\n",
    "    \"\"\"Compare advanced models with baseline models from previous notebook.\"\"\"\n",
    "    \n",
    "    print(\"\\n📈 ADVANCED VS BASELINE COMPARISON\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Try to load baseline results\n",
    "    baseline_file = RESULTS_DIR / 'baseline_model_results.json'\n",
    "    \n",
    "    if baseline_file.exists():\n",
    "        try:\n",
    "            baseline_results = pd.read_json(baseline_file)\n",
    "            print(\"✅ Loaded baseline model results\")\n",
    "            \n",
    "            # Create comparison\n",
    "            comparison_data = []\n",
    "            \n",
    "            for horizon in PREDICTION_HORIZONS:\n",
    "                horizon_str = f\"{horizon}d\"\n",
    "                \n",
    "                # Advanced models\n",
    "                lgb_key = f'lightgbm_{horizon_str}'\n",
    "                xgb_key = f'xgboost_{horizon_str}'\n",
    "                \n",
    "                if lgb_key in advanced_results and xgb_key in advanced_results:\n",
    "                    lgb_rmse = advanced_results[lgb_key].get('test_rmse', np.nan)\n",
    "                    xgb_rmse = advanced_results[xgb_key].get('test_rmse', np.nan)\n",
    "                    \n",
    "                    comparison_data.append({\n",
    "                        'Horizon': horizon_str,\n",
    "                        'LightGBM_RMSE': lgb_rmse,\n",
    "                        'XGBoost_RMSE': xgb_rmse,\n",
    "                        'Best_Advanced': min(lgb_rmse, xgb_rmse) if not (np.isnan(lgb_rmse) or np.isnan(xgb_rmse)) else np.nan\n",
    "                    })\n",
    "            \n",
    "            comparison_df = pd.DataFrame(comparison_data)\n",
    "            print(comparison_df.to_string(index=False, float_format='%.4f'))\n",
    "            \n",
    "            return comparison_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error loading baseline results: {e}\")\n",
    "    \n",
    "    else:\n",
    "        print(\"⚠️ Baseline results not found - run notebook 04 first\")\n",
    "    \n",
    "    return pd.DataFrame()\n",
    "\n",
    "# Compare models\n",
    "if trainer.results:\n",
    "    model_comparison = compare_with_baseline_models(trainer.results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17624361",
   "metadata": {},
   "source": [
    "### 8- PREDICTION VISUALIZATION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd57f2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(trainer: AdvancedModelTrainer, data: pd.DataFrame, \n",
    "                        horizon: int = 7, sample_size: int = 100):\n",
    "    \"\"\"Visualize model predictions vs actual values.\"\"\"\n",
    "    \n",
    "    model_lgb = trainer.models.get(f'lightgbm_{horizon}d')\n",
    "    model_xgb = trainer.models.get(f'xgboost_{horizon}d')\n",
    "    \n",
    "    if not (model_lgb and model_xgb):\n",
    "        print(f\"❌ Models not available for {horizon}d horizon\")\n",
    "        return\n",
    "    \n",
    "    # Prepare data\n",
    "    X, y = trainer.prepare_model_data(data, horizon)\n",
    "    \n",
    "    # Get recent sample\n",
    "    X_sample = X.tail(sample_size)\n",
    "    y_sample = y.tail(sample_size)\n",
    "    \n",
    "    # Make predictions\n",
    "    try:\n",
    "        pred_lgb = model_lgb.predict(X_sample)\n",
    "        pred_xgb = model_xgb.predict(X_sample)\n",
    "        \n",
    "        # Create visualization\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Predictions vs Actual\n",
    "        axes[0, 0].scatter(y_sample, pred_lgb, alpha=0.6, label='LightGBM')\n",
    "        axes[0, 0].plot([y_sample.min(), y_sample.max()], \n",
    "                        [y_sample.min(), y_sample.max()], 'r--', alpha=0.8)\n",
    "        axes[0, 0].set_xlabel('Actual')\n",
    "        axes[0, 0].set_ylabel('Predicted')\n",
    "        axes[0, 0].set_title(f'LightGBM - {horizon}d Predictions vs Actual')\n",
    "        axes[0, 0].legend()\n",
    "        \n",
    "        axes[0, 1].scatter(y_sample, pred_xgb, alpha=0.6, label='XGBoost', color='orange')\n",
    "        axes[0, 1].plot([y_sample.min(), y_sample.max()], \n",
    "                        [y_sample.min(), y_sample.max()], 'r--', alpha=0.8)\n",
    "        axes[0, 1].set_xlabel('Actual')\n",
    "        axes[0, 1].set_ylabel('Predicted')\n",
    "        axes[0, 1].set_title(f'XGBoost - {horizon}d Predictions vs Actual')\n",
    "        axes[0, 1].legend()\n",
    "        \n",
    "        # Time series plots\n",
    "        dates = X_sample.index\n",
    "        axes[1, 0].plot(dates, y_sample, label='Actual', color='black', linewidth=2)\n",
    "        axes[1, 0].plot(dates, pred_lgb, label='LightGBM', alpha=0.8)\n",
    "        axes[1, 0].set_title(f'LightGBM - {horizon}d Time Series')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        axes[1, 1].plot(dates, y_sample, label='Actual', color='black', linewidth=2)\n",
    "        axes[1, 1].plot(dates, pred_xgb, label='XGBoost', alpha=0.8, color='orange')\n",
    "        axes[1, 1].set_title(f'XGBoost - {horizon}d Time Series')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(FIGURES_DIR / f'advanced_models_predictions_{horizon}d.png', \n",
    "                   dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # Calculate and display metrics\n",
    "        lgb_rmse = calculate_rmse(y_sample, pred_lgb)\n",
    "        xgb_rmse = calculate_rmse(y_sample, pred_xgb)\n",
    "        lgb_mae = calculate_mae(y_sample, pred_lgb)\n",
    "        xgb_mae = calculate_mae(y_sample, pred_xgb)\n",
    "        \n",
    "        print(f\"\\n📊 PREDICTION QUALITY - {horizon}d Horizon (Last {sample_size} samples)\")\n",
    "        print(f\"LightGBM - RMSE: {lgb_rmse:.4f}, MAE: {lgb_mae:.4f}\")\n",
    "        print(f\"XGBoost  - RMSE: {xgb_rmse:.4f}, MAE: {xgb_mae:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in prediction visualization: {e}\")\n",
    "        print(f\"❌ Visualization error: {e}\")\n",
    "\n",
    "# Visualize predictions for 7-day horizon\n",
    "print(\"\\n📊 VISUALIZING PREDICTIONS\")\n",
    "if trainer.models:\n",
    "    visualize_predictions(trainer, sample_data, horizon=7, sample_size=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fee597",
   "metadata": {},
   "source": [
    "### 9- MODEL PERSISTENCE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe12686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_advanced_models(trainer: AdvancedModelTrainer):\n",
    "    \"\"\"Save trained models and results.\"\"\"\n",
    "    \n",
    "    print(\"\\n💾 SAVING MODELS AND RESULTS\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    try:\n",
    "        # Save model results\n",
    "        results_file = RESULTS_DIR / 'advanced_model_results.json'\n",
    "        save_results_to_json(trainer.results, results_file)\n",
    "        print(f\"✅ Results saved to {results_file}\")\n",
    "        \n",
    "        # Save models using joblib\n",
    "        models_saved = 0\n",
    "        for model_name, model in trainer.models.items():\n",
    "            model_file = MODELS_DIR / f\"{model_name}.joblib\"\n",
    "            joblib.dump(model, model_file)\n",
    "            models_saved += 1\n",
    "        \n",
    "        print(f\"✅ Saved {models_saved} models to {MODELS_DIR}\")\n",
    "        \n",
    "        # Save feature importance\n",
    "        if trainer.feature_importance:\n",
    "            importance_file = RESULTS_DIR / 'feature_importance_advanced.json'\n",
    "            importance_dict = {}\n",
    "            for model_name, df in trainer.feature_importance.items():\n",
    "                importance_dict[model_name] = df.to_dict('records')\n",
    "            \n",
    "            save_results_to_json(importance_dict, importance_file)\n",
    "            print(f\"✅ Feature importance saved to {importance_file}\")\n",
    "        \n",
    "        # Create model records for tracking\n",
    "        model_records = []\n",
    "        for model_name, model in trainer.models.items():\n",
    "            model_type = model_name.split('_')[0]\n",
    "            horizon = model_name.split('_')[1]\n",
    "            \n",
    "            record = {\n",
    "                'model_name': model_name,\n",
    "                'model_type': model_type,\n",
    "                'horizon': horizon,\n",
    "                'created_at': datetime.now().isoformat(),\n",
    "                'model_file': f\"{model_name}.joblib\",\n",
    "                'performance': trainer.results.get(model_name, {})\n",
    "            }\n",
    "            model_records.append(record)\n",
    "        \n",
    "        records_file = RESULTS_DIR / 'advanced_model_records.json'\n",
    "        save_results_to_json(model_records, records_file)\n",
    "        print(f\"✅ Model records saved to {records_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving models: {e}\")\n",
    "        print(f\"❌ Save error: {e}\")\n",
    "\n",
    "# Save models and results\n",
    "if trainer.models and trainer.results:\n",
    "    save_advanced_models(trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15d13e1",
   "metadata": {},
   "source": [
    "### 10- SUMMARY AND NEXT STEPS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74354b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"📋 ADVANCED MODELS TRAINING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if trainer.results:\n",
    "    print(f\"✅ Models trained: {len(trainer.models)}\")\n",
    "    print(f\"✅ Horizons covered: {PREDICTION_HORIZONS}\")\n",
    "    print(f\"✅ Results generated: {len(trainer.results)}\")\n",
    "    \n",
    "    # Best performing models\n",
    "    print(\"\\n🏆 BEST PERFORMING MODELS BY HORIZON:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for horizon in PREDICTION_HORIZONS:\n",
    "        horizon_str = f\"{horizon}d\"\n",
    "        lgb_key = f'lightgbm_{horizon_str}'\n",
    "        xgb_key = f'xgboost_{horizon_str}'\n",
    "        \n",
    "        lgb_rmse = trainer.results.get(lgb_key, {}).get('test_rmse', float('inf'))\n",
    "        xgb_rmse = trainer.results.get(xgb_key, {}).get('test_rmse', float('inf'))\n",
    "        \n",
    "        if lgb_rmse < xgb_rmse and lgb_rmse != float('inf'):\n",
    "            best_model = \"LightGBM\"\n",
    "            best_rmse = lgb_rmse\n",
    "        elif xgb_rmse != float('inf'):\n",
    "            best_model = \"XGBoost\"\n",
    "            best_rmse = xgb_rmse\n",
    "        else:\n",
    "            best_model = \"N/A\"\n",
    "            best_rmse = \"N/A\"\n",
    "        \n",
    "        print(f\"{horizon_str:>3} horizon: {best_model:>8} (RMSE: {best_rmse})\")\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️ No models were successfully trained\")\n",
    "\n",
    "print(f\"\\n📁 Results saved to: {RESULTS_DIR}\")\n",
    "print(f\"📁 Models saved to: {MODELS_DIR}\")\n",
    "print(f\"📁 Figures saved to: {FIGURES_DIR}\")\n",
    "\n",
    "print(\"\\n🚀 NEXT STEPS:\")\n",
    "print(\"1. Run notebook 06 for LSTM deep learning models (optional)\")\n",
    "print(\"2. Run notebook 07 for comprehensive evaluation and backtesting\")\n",
    "print(\"3. Use trained models in the Streamlit app\")\n",
    "\n",
    "print(f\"\\n✅ Advanced models notebook completed successfully!\")\n",
    "print(f\"⏰ Execution time: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05c26a0",
   "metadata": {},
   "source": [
    "### 11- OPTIONAL: QUICK MODEL TESTING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1511d8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_model_test():\n",
    "    \"\"\"Quick test of trained models with recent data.\"\"\"\n",
    "    \n",
    "    if not trainer.models:\n",
    "        print(\"⚠️ No models available for testing\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n🧪 QUICK MODEL TEST\")\n",
    "    print(\"-\" * 20)\n",
    "    \n",
    "    try:\n",
    "        # Get recent data for testing\n",
    "        test_data = sample_data.tail(10)\n",
    "        \n",
    "        for model_name, model in trainer.models.items():\n",
    "            horizon = int(model_name.split('_')[1].replace('d', ''))\n",
    "            X, y = trainer.prepare_model_data(sample_data, horizon)\n",
    "            \n",
    "            # Make prediction on last available data point\n",
    "            last_features = X.tail(1)\n",
    "            if not last_features.empty:\n",
    "                prediction = model.predict(last_features)[0]\n",
    "                print(f\"{model_name:>15}: {prediction:>8.4f}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Test error: {e}\")\n",
    "\n",
    "# Run quick test\n",
    "# quick_model_test()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🎉 NOTEBOOK EXECUTION COMPLETED\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stock-predictor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
