{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e360411b",
   "metadata": {},
   "source": [
    "### ðŸ“Š Model Evaluation and Backtesting\n",
    " \n",
    "### **Purpose:** Final model evaluation and comprehensive backtesting\n",
    " \n",
    "### This notebook provides:\n",
    "- Loading and comparison of all trained models\n",
    "- Comprehensive performance evaluation across multiple metrics\n",
    "- Backtesting simulation with trading strategies\n",
    "- Final performance reports and visualizations\n",
    "- Statistical significance testing\n",
    "- Model selection recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ead8169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "import os\n",
    "import pickle\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Project imports\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.utils import (\n",
    "    get_project_root, get_project_path, load_results_from_json,\n",
    "    generate_performance_report, create_html_report, plot_model_comparison,\n",
    "    plot_predictions_vs_actual, create_time_series_plot, save_results_to_json\n",
    ")\n",
    "from src.data import (\n",
    "    load_config, setup_logging, load_raw_data, get_default_tickers,\n",
    "    clean_data, calculate_returns\n",
    ")\n",
    "from src.models import (\n",
    "    BasePredictor, NaiveLastValue, RandomWalkDrift, \n",
    "    RandomForestPredictor, LightGBMPredictor, XGBPredictor, LSTMPredictor,\n",
    "    load_model_records, predict_multi_horizon\n",
    ")\n",
    "from src.evaluate import (\n",
    "    compare_models, create_evaluation_report, walk_forward_validation,\n",
    "    calculate_rmse, calculate_mae, calculate_mape, calculate_r2,\n",
    "    calculate_profit_loss, calculate_max_drawdown, evaluate_trading_strategy,\n",
    "    statistical_significance_test, plot_validation_results\n",
    ")\n",
    "from src.features import (\n",
    "    create_all_features, scale_features, create_targets,\n",
    "    process_stock_features\n",
    ")\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# %%\n",
    "# Setup paths and logging\n",
    "project_root = get_project_root()\n",
    "config_path = get_project_path(\"config/config.yaml\")\n",
    "models_dir = get_project_path(\"experiments/models\")\n",
    "results_dir = get_project_path(\"experiments/results\")\n",
    "figures_dir = get_project_path(\"experiments/figures\")\n",
    "\n",
    "# Load configuration\n",
    "config = load_config(config_path)\n",
    "logger = setup_logging()\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Models directory: {models_dir}\")\n",
    "print(f\"Results directory: {results_dir}\")\n",
    "print(f\"Configuration loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05387ba2",
   "metadata": {},
   "source": [
    "### ðŸ“‚ Load Available Models and Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c2170b",
   "metadata": {},
   "source": [
    "def load_saved_models():\n",
    "    \"\"\"Load all saved models from the models directory.\"\"\"\n",
    "    models = {}\n",
    "    model_files = list(Path(models_dir).glob(\"*.pkl\")) + list(Path(models_dir).glob(\"*.joblib\"))\n",
    "    \n",
    "    print(f\"Found {len(model_files)} model files:\")\n",
    "    for model_file in model_files:\n",
    "        print(f\"  - {model_file.name}\")\n",
    "        \n",
    "        try:\n",
    "            # Determine loading method based on file extension\n",
    "            if model_file.suffix == '.pkl':\n",
    "                with open(model_file, 'rb') as f:\n",
    "                    model = pickle.load(f)\n",
    "            else:  # .joblib\n",
    "                model = joblib.load(model_file)\n",
    "            \n",
    "            # Extract model name from filename\n",
    "            model_name = model_file.stem\n",
    "            models[model_name] = model\n",
    "            print(f\"    âœ“ Loaded successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    âœ— Failed to load: {e}\")\n",
    "    \n",
    "    return models\n",
    "\n",
    "# Load all available models\n",
    "saved_models = load_saved_models()\n",
    "print(f\"\\nTotal models loaded: {len(saved_models)}\")\n",
    "\n",
    "\n",
    "def load_saved_results():\n",
    "    \"\"\"Load all saved evaluation results.\"\"\"\n",
    "    results = {}\n",
    "    result_files = list(Path(results_dir).glob(\"*.json\"))\n",
    "    \n",
    "    print(f\"Found {len(result_files)} result files:\")\n",
    "    for result_file in result_files:\n",
    "        print(f\"  - {result_file.name}\")\n",
    "        \n",
    "        try:\n",
    "            result = load_results_from_json(str(result_file))\n",
    "            result_name = result_file.stem\n",
    "            results[result_name] = result\n",
    "            print(f\"    âœ“ Loaded successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"    âœ— Failed to load: {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Load all available results\n",
    "saved_results = load_saved_results()\n",
    "print(f\"\\nTotal results loaded: {len(saved_results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e666b335",
   "metadata": {},
   "source": [
    "### ðŸ“Š Data Preparation for Backtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f92a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data for backtesting\n",
    "tickers = get_default_tickers()\n",
    "test_ticker = config['data']['primary_ticker']\n",
    "\n",
    "print(f\"Loading data for backtesting ticker: {test_ticker}\")\n",
    "\n",
    "# Load raw data\n",
    "raw_data = load_raw_data(get_project_path(f\"data/raw/{test_ticker}.csv\"))\n",
    "print(f\"Raw data shape: {raw_data.shape}\")\n",
    "print(f\"Date range: {raw_data.index.min()} to {raw_data.index.max()}\")\n",
    "\n",
    "# Clean data\n",
    "cleaned_data = clean_data(raw_data)\n",
    "print(f\"Cleaned data shape: {cleaned_data.shape}\")\n",
    "\n",
    "# Create features\n",
    "features_data = create_all_features(\n",
    "    cleaned_data, \n",
    "    config['features']['technical_indicators'],\n",
    "    config['features']['lag_features'],\n",
    "    config['features']['rolling_features']\n",
    ")\n",
    "print(f\"Features data shape: {features_data.shape}\")\n",
    "\n",
    "# Define backtesting period (last 252 trading days = ~1 year)\n",
    "backtest_start = features_data.index[-252]\n",
    "backtest_data = features_data[features_data.index >= backtest_start].copy()\n",
    "print(f\"Backtesting period: {backtest_start} to {features_data.index.max()}\")\n",
    "print(f\"Backtesting data shape: {backtest_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c378089b",
   "metadata": {},
   "source": [
    "### ðŸ”„ Model Reconstruction and Backtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f702707d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_baseline_models():\n",
    "    \"\"\"Create baseline models for comparison.\"\"\"\n",
    "    return {\n",
    "        'naive_last_value': NaiveLastValue(),\n",
    "        'random_walk_drift': RandomWalkDrift()\n",
    "    }\n",
    "\n",
    "def backtest_model(model, data, target_col='close', horizon=1):\n",
    "    \"\"\"\n",
    "    Perform walk-forward backtesting on a model.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model instance\n",
    "        data: DataFrame with features and target\n",
    "        target_col: Target column name\n",
    "        horizon: Prediction horizon\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with predictions and actual values\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    actual_values = []\n",
    "    dates = []\n",
    "    \n",
    "    # Minimum training window\n",
    "    min_train_size = 252  # 1 year of data\n",
    "    \n",
    "    for i in range(min_train_size, len(data) - horizon + 1):\n",
    "        # Training data\n",
    "        train_data = data.iloc[:i]\n",
    "        \n",
    "        # Target date and value\n",
    "        target_date = data.index[i + horizon - 1]\n",
    "        target_value = data.iloc[i + horizon - 1][target_col]\n",
    "        \n",
    "        try:\n",
    "            # Make prediction\n",
    "            if hasattr(model, 'predict'):\n",
    "                # For sklearn-style models\n",
    "                feature_cols = [col for col in train_data.columns if col != target_col]\n",
    "                X = train_data[feature_cols].iloc[[-1]]  # Last row for prediction\n",
    "                pred = model.predict(X)[0]\n",
    "            else:\n",
    "                # For custom baseline models\n",
    "                pred = model.predict(train_data[target_col].values, steps=horizon)\n",
    "                if isinstance(pred, (list, np.ndarray)):\n",
    "                    pred = pred[-1]\n",
    "            \n",
    "            predictions.append(pred)\n",
    "            actual_values.append(target_value)\n",
    "            dates.append(target_date)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error predicting for date {target_date}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    results_df = pd.DataFrame({\n",
    "        'date': dates,\n",
    "        'actual': actual_values,\n",
    "        'predicted': predictions\n",
    "    })\n",
    "    results_df.set_index('date', inplace=True)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Create baseline models\n",
    "baseline_models = create_baseline_models()\n",
    "all_models = {**baseline_models, **saved_models}\n",
    "\n",
    "print(f\"Total models for backtesting: {len(all_models)}\")\n",
    "for name in all_models.keys():\n",
    "    print(f\"  - {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e2f5f3",
   "metadata": {},
   "source": [
    "### ðŸ“ˆ Comprehensive Backtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1371e296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features for backtesting\n",
    "feature_columns = [col for col in backtest_data.columns if col not in ['close', 'target_1d', 'target_5d']]\n",
    "target_column = 'close'\n",
    "\n",
    "print(f\"Using {len(feature_columns)} features for backtesting\")\n",
    "print(f\"Target column: {target_column}\")\n",
    "\n",
    "# Scale features if needed\n",
    "scaled_features = scale_features(backtest_data[feature_columns])\n",
    "backtest_data_scaled = pd.concat([scaled_features, backtest_data[['close']]], axis=1)\n",
    "\n",
    "# Perform backtesting for all models\n",
    "backtest_results = {}\n",
    "model_performance = {}\n",
    "\n",
    "print(\"Starting comprehensive backtesting...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for model_name, model in all_models.items():\n",
    "    print(f\"\\nðŸ”„ Backtesting {model_name}...\")\n",
    "    \n",
    "    try:\n",
    "        # Choose appropriate data based on model type\n",
    "        if model_name in ['naive_last_value', 'random_walk_drift']:\n",
    "            # Baseline models don't need scaled features\n",
    "            data_for_model = backtest_data\n",
    "        else:\n",
    "            # ML models typically need scaled features\n",
    "            data_for_model = backtest_data_scaled\n",
    "        \n",
    "        # Perform backtesting\n",
    "        results = backtest_model(model, data_for_model, target_column, horizon=1)\n",
    "        \n",
    "        if len(results) > 0:\n",
    "            backtest_results[model_name] = results\n",
    "            \n",
    "            # Calculate performance metrics\n",
    "            actual = results['actual'].values\n",
    "            predicted = results['predicted'].values\n",
    "            \n",
    "            metrics = {\n",
    "                'rmse': calculate_rmse(actual, predicted),\n",
    "                'mae': calculate_mae(actual, predicted),\n",
    "                'mape': calculate_mape(actual, predicted),\n",
    "                'r2': calculate_r2(actual, predicted),\n",
    "                'num_predictions': len(results)\n",
    "            }\n",
    "            \n",
    "            model_performance[model_name] = metrics\n",
    "            \n",
    "            print(f\"  âœ“ Completed: {len(results)} predictions\")\n",
    "            print(f\"    RMSE: {metrics['rmse']:.4f}\")\n",
    "            print(f\"    MAE: {metrics['mae']:.4f}\")\n",
    "            print(f\"    RÂ²: {metrics['r2']:.4f}\")\n",
    "        else:\n",
    "            print(f\"  âœ— No valid predictions generated\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  âœ— Error: {e}\")\n",
    "\n",
    "print(f\"\\nâœ… Backtesting completed for {len(backtest_results)} models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ab03cb",
   "metadata": {},
   "source": [
    "### ðŸ“Š Performance Comparison and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e76e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create performance comparison DataFrame\n",
    "if model_performance:\n",
    "    performance_df = pd.DataFrame(model_performance).T\n",
    "    performance_df = performance_df.round(4)\n",
    "    \n",
    "    print(\"ðŸ† Model Performance Ranking\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Sort by RÂ² (descending) and RMSE (ascending)\n",
    "    performance_df_sorted = performance_df.sort_values(['r2', 'rmse'], ascending=[False, True])\n",
    "    print(performance_df_sorted)\n",
    "    \n",
    "    # Save performance results\n",
    "    performance_results = {\n",
    "        'backtest_period': {\n",
    "            'start': str(backtest_data.index.min()),\n",
    "            'end': str(backtest_data.index.max()),\n",
    "            'num_days': len(backtest_data)\n",
    "        },\n",
    "        'model_performance': performance_df.to_dict(),\n",
    "        'ranking': performance_df_sorted.index.tolist()\n",
    "    }\n",
    "    \n",
    "    save_results_to_json(\n",
    "        performance_results, \n",
    "        get_project_path(\"experiments/results/backtest_performance.json\")\n",
    "    )\n",
    "    print(\"\\nðŸ’¾ Performance results saved\")\n",
    "\n",
    "\n",
    "# Visualize model performance comparison\n",
    "if len(model_performance) > 1:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('ðŸ“Š Model Performance Comparison - Backtesting Results', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    metrics = ['rmse', 'mae', 'mape', 'r2']\n",
    "    metric_names = ['RMSE', 'MAE', 'MAPE (%)', 'RÂ²']\n",
    "    \n",
    "    for idx, (metric, name) in enumerate(zip(metrics, metric_names)):\n",
    "        ax = axes[idx // 2, idx % 2]\n",
    "        \n",
    "        # Extract values\n",
    "        models = list(model_performance.keys())\n",
    "        values = [model_performance[model][metric] for model in models]\n",
    "        \n",
    "        # Create bar plot\n",
    "        bars = ax.bar(models, values)\n",
    "        ax.set_title(f'{name} Comparison', fontweight='bold')\n",
    "        ax.set_ylabel(name)\n",
    "        \n",
    "        # Rotate x-labels for better readability\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, value in zip(bars, values):\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                   f'{value:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "        # Color the best performing bar\n",
    "        if metric == 'r2':\n",
    "            best_idx = np.argmax(values)\n",
    "        else:\n",
    "            best_idx = np.argmin(values)\n",
    "        bars[best_idx].set_color('gold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(get_project_path(\"experiments/figures/backtest_performance_comparison.png\"), \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d127f12",
   "metadata": {},
   "source": [
    "### ðŸ“ˆ Prediction vs Actual Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdab2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot prediction vs actual for top 3 models\n",
    "if backtest_results:\n",
    "    top_models = performance_df_sorted.head(3).index.tolist()\n",
    "    \n",
    "    fig, axes = plt.subplots(len(top_models), 1, figsize=(15, 5 * len(top_models)))\n",
    "    if len(top_models) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    fig.suptitle('ðŸŽ¯ Top Models: Predictions vs Actual Prices', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    for idx, model_name in enumerate(top_models):\n",
    "        ax = axes[idx]\n",
    "        results = backtest_results[model_name]\n",
    "        \n",
    "        # Plot actual vs predicted\n",
    "        ax.plot(results.index, results['actual'], label='Actual', color='blue', linewidth=2)\n",
    "        ax.plot(results.index, results['predicted'], label='Predicted', color='red', linewidth=1, alpha=0.8)\n",
    "        \n",
    "        # Add performance metrics to title\n",
    "        metrics = model_performance[model_name]\n",
    "        title = f\"{model_name} | RÂ²: {metrics['r2']:.3f} | RMSE: {metrics['rmse']:.3f} | MAE: {metrics['mae']:.3f}\"\n",
    "        ax.set_title(title, fontweight='bold')\n",
    "        \n",
    "        ax.set_ylabel('Price ($)')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Format x-axis\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\n",
    "        get_project_path(\"experiments/figures/top_models_predictions.png\"),\n",
    "        dpi=300,\n",
    "        bbox_inches='tight'\n",
    "    )\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5b2237",
   "metadata": {},
   "source": [
    "### ðŸ’° Trading Strategy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1042b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_trading_performance(results_df, initial_capital=10000):\n",
    "    \"\"\"\n",
    "    Evaluate trading performance based on prediction signals.\n",
    "    \n",
    "    Args:\n",
    "        results_df: DataFrame with actual and predicted prices\n",
    "        initial_capital: Starting capital\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with trading performance metrics\n",
    "    \"\"\"\n",
    "    df = results_df.copy()\n",
    "    \n",
    "    # Calculate returns\n",
    "    df['actual_return'] = df['actual'].pct_change()\n",
    "    df['predicted_return'] = df['predicted'].pct_change()\n",
    "    \n",
    "    # Generate signals (1 for buy, -1 for sell, 0 for hold)\n",
    "    df['signal'] = np.where(\n",
    "        df['predicted_return'] > 0.001, 1,   # Buy if predicted return > 0.1%\n",
    "        np.where(df['predicted_return'] < -0.001, -1, 0)  # Sell if predicted return < -0.1%\n",
    "    )\n",
    "    \n",
    "    # Calculate strategy returns\n",
    "    df['strategy_return'] = df['signal'].shift(1) * df['actual_return']\n",
    "    df['strategy_return'] = df['strategy_return'].fillna(0)\n",
    "    \n",
    "    # Calculate cumulative returns\n",
    "    df['actual_cumulative'] = (1 + df['actual_return']).cumprod()\n",
    "    df['strategy_cumulative'] = (1 + df['strategy_return']).cumprod()\n",
    "    \n",
    "    # Calculate portfolio values\n",
    "    df['buy_hold_value'] = initial_capital * df['actual_cumulative']\n",
    "    df['strategy_value'] = initial_capital * df['strategy_cumulative']\n",
    "    \n",
    "    # Performance metrics\n",
    "    total_return_bh = (df['buy_hold_value'].iloc[-1] / initial_capital - 1) * 100\n",
    "    total_return_strategy = (df['strategy_value'].iloc[-1] / initial_capital - 1) * 100\n",
    "    \n",
    "    # Calculate Sharpe ratios (assuming 252 trading days per year)\n",
    "    sharpe_bh = (df['actual_return'].mean() * 252) / (df['actual_return'].std() * np.sqrt(252))\n",
    "    sharpe_strategy = (df['strategy_return'].mean() * 252) / (df['strategy_return'].std() * np.sqrt(252))\n",
    "    \n",
    "    # Maximum drawdown\n",
    "    rolling_max_bh = df['buy_hold_value'].expanding().max()\n",
    "    drawdown_bh = ((df['buy_hold_value'] - rolling_max_bh) / rolling_max_bh * 100)\n",
    "    max_drawdown_bh = drawdown_bh.min()\n",
    "    \n",
    "    rolling_max_strategy = df['strategy_value'].expanding().max()\n",
    "    drawdown_strategy = ((df['strategy_value'] - rolling_max_strategy) / rolling_max_strategy * 100)\n",
    "    max_drawdown_strategy = drawdown_strategy.min()\n",
    "    \n",
    "    # Win rate\n",
    "    winning_trades = (df['strategy_return'] > 0).sum()\n",
    "    total_trades = (df['strategy_return'] != 0).sum()\n",
    "    win_rate = (winning_trades / total_trades * 100) if total_trades > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'total_return_buy_hold': total_return_bh,\n",
    "        'total_return_strategy': total_return_strategy,\n",
    "        'excess_return': total_return_strategy - total_return_bh,\n",
    "        'sharpe_ratio_buy_hold': sharpe_bh,\n",
    "        'sharpe_ratio_strategy': sharpe_strategy,\n",
    "        'max_drawdown_buy_hold': max_drawdown_bh,\n",
    "        'max_drawdown_strategy': max_drawdown_strategy,\n",
    "        'win_rate': win_rate,\n",
    "        'total_trades': total_trades,\n",
    "        'final_value_buy_hold': df['buy_hold_value'].iloc[-1],\n",
    "        'final_value_strategy': df['strategy_value'].iloc[-1],\n",
    "        'portfolio_data': df\n",
    "    }\n",
    "\n",
    "# Evaluate trading performance for top models\n",
    "trading_performance = {}\n",
    "\n",
    "print(\"ðŸ’° Trading Strategy Evaluation\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for model_name in top_models:\n",
    "    print(f\"\\nðŸ“Š Evaluating {model_name}...\")\n",
    "    \n",
    "    results = backtest_results[model_name]\n",
    "    trading_metrics = evaluate_trading_performance(results, initial_capital=10000)\n",
    "    trading_performance[model_name] = trading_metrics\n",
    "    \n",
    "    print(f\"  Buy & Hold Return: {trading_metrics['total_return_buy_hold']:.2f}%\")\n",
    "    print(f\"  Strategy Return: {trading_metrics['total_return_strategy']:.2f}%\")\n",
    "    print(f\"  Excess Return: {trading_metrics['excess_return']:.2f}%\")\n",
    "    print(f\"  Sharpe Ratio (Strategy): {trading_metrics['sharpe_ratio_strategy']:.3f}\")\n",
    "    print(f\"  Max Drawdown: {trading_metrics['max_drawdown_strategy']:.2f}%\")\n",
    "    print(f\"  Win Rate: {trading_metrics['win_rate']:.1f}%\")\n",
    "    print(f\"  Total Trades: {trading_metrics['total_trades']}\")\n",
    "\n",
    "# Visualize trading performance\n",
    "if trading_performance:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('ðŸ’° Trading Strategy Performance Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Cumulative returns comparison\n",
    "    ax1 = axes[0, 0]\n",
    "    for model_name in top_models:\n",
    "        portfolio_data = trading_performance[model_name]['portfolio_data']\n",
    "        ax1.plot(\n",
    "            portfolio_data.index, portfolio_data['buy_hold_value'],\n",
    "            label=f'{model_name} - Buy & Hold', linestyle='--', alpha=0.7\n",
    "        )\n",
    "        ax1.plot(\n",
    "            portfolio_data.index, portfolio_data['strategy_value'],\n",
    "            label=f'{model_name} - Strategy', linewidth=2\n",
    "        )\n",
    "    \n",
    "    ax1.set_title('Portfolio Value Over Time', fontweight='bold')\n",
    "    ax1.set_ylabel('Portfolio Value ($)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Total returns comparison\n",
    "    ax2 = axes[0, 1]\n",
    "    models = list(trading_performance.keys())\n",
    "    bh_returns = [trading_performance[m]['total_return_buy_hold'] for m in models]\n",
    "    strategy_returns = [trading_performance[m]['total_return_strategy'] for m in models]\n",
    "    \n",
    "    x = np.arange(len(models))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax2.bar(x - width / 2, bh_returns, width, label='Buy & Hold', alpha=0.7)\n",
    "    bars2 = ax2.bar(x + width / 2, strategy_returns, width, label='Strategy')\n",
    "    \n",
    "    ax2.set_title('Total Returns Comparison', fontweight='bold')\n",
    "    ax2.set_ylabel('Total Return (%)')\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels(models, rotation=45)\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, value in zip(bars1, bh_returns):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width() / 2., height + 0.5,\n",
    "                 f'{value:.1f}%', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    for bar, value in zip(bars2, strategy_returns):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width() / 2., height + 0.5,\n",
    "                 f'{value:.1f}%', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # 3. Risk-adjusted returns (Sharpe ratio)\n",
    "    ax3 = axes[1, 0]\n",
    "    sharpe_bh = [trading_performance[m]['sharpe_ratio_buy_hold'] for m in models]\n",
    "    sharpe_strategy = [trading_performance[m]['sharpe_ratio_strategy'] for m in models]\n",
    "    \n",
    "    bars3 = ax3.bar(x - width / 2, sharpe_bh, width, label='Buy & Hold', alpha=0.7)\n",
    "    bars4 = ax3.bar(x + width / 2, sharpe_strategy, width, label='Strategy')\n",
    "    \n",
    "    ax3.set_title('Sharpe Ratio Comparison', fontweight='bold')\n",
    "    ax3.set_ylabel('Sharpe Ratio')\n",
    "    ax3.set_xticks(x)\n",
    "    ax3.set_xticklabels(models, rotation=45)\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Maximum drawdown\n",
    "    ax4 = axes[1, 1]\n",
    "    drawdown_bh = [abs(trading_performance[m]['max_drawdown_buy_hold']) for m in models]\n",
    "    drawdown_strategy = [abs(trading_performance[m]['max_drawdown_strategy']) for m in models]\n",
    "    \n",
    "    bars5 = ax4.bar(x - width / 2, drawdown_bh, width, label='Buy & Hold', alpha=0.7)\n",
    "    bars6 = ax4.bar(x + width / 2, drawdown_strategy, width, label='Strategy')\n",
    "    \n",
    "    ax4.set_title('Maximum Drawdown Comparison', fontweight='bold')\n",
    "    ax4.set_ylabel('Maximum Drawdown (%)')\n",
    "    ax4.set_xticks(x)\n",
    "    ax4.set_xticklabels(models, rotation=45)\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\n",
    "        get_project_path(\"experiments/figures/trading_performance_analysis.png\"),\n",
    "        dpi=300,\n",
    "        bbox_inches='tight'\n",
    "    )\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be28828",
   "metadata": {},
   "source": [
    "### ðŸ“Š Statistical Significance Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e2cb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform statistical significance tests\n",
    "print(\"ðŸ”¬ Statistical Significance Testing\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "if len(backtest_results) >= 2:\n",
    "    model_names = list(backtest_results.keys())\n",
    "    \n",
    "    # Pairwise comparison of top models\n",
    "    for i in range(len(top_models)):\n",
    "        for j in range(i + 1, len(top_models)):\n",
    "            model1 = top_models[i]\n",
    "            model2 = top_models[j]\n",
    "            \n",
    "            if model1 in backtest_results and model2 in backtest_results:\n",
    "                results1 = backtest_results[model1]\n",
    "                results2 = backtest_results[model2]\n",
    "                \n",
    "                # Align dates for comparison\n",
    "                common_dates = results1.index.intersection(results2.index)\n",
    "                if len(common_dates) > 10:  # Need minimum samples\n",
    "                    actual1 = results1.loc[common_dates, 'actual']\n",
    "                    pred1 = results1.loc[common_dates, 'predicted']\n",
    "                    pred2 = results2.loc[common_dates, 'predicted']\n",
    "                    \n",
    "                    # Calculate prediction errors\n",
    "                    errors1 = np.abs(actual1 - pred1)\n",
    "                    errors2 = np.abs(actual1 - pred2)\n",
    "                    \n",
    "                    # Perform statistical significance test\n",
    "                    stat_result = statistical_significance_test(errors1.values, errors2.values)\n",
    "                    \n",
    "                    print(f\"\\n{model1} vs {model2}:\")\n",
    "                    print(f\"  Mean MAE {model1}: {errors1.mean():.4f}\")\n",
    "                    print(f\"  Mean MAE {model2}: {errors2.mean():.4f}\")\n",
    "                    print(f\"  P-value: {stat_result['p_value']:.4f}\")\n",
    "                    print(f\"  Significant difference: {stat_result['is_significant']}\")\n",
    "                    if stat_result['is_significant']:\n",
    "                        better_model = model1 if errors1.mean() < errors2.mean() else model2\n",
    "                        print(f\"  Better model: {better_model}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c9abbd",
   "metadata": {},
   "source": [
    "### ðŸ“‹ Final Performance Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24134cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive performance report\n",
    "report_data = {\n",
    "    'evaluation_metadata': {\n",
    "        'evaluation_date': datetime.now().isoformat(),\n",
    "        'backtest_period_start': str(backtest_data.index.min()),\n",
    "        'backtest_period_end': str(backtest_data.index.max()),\n",
    "        'backtest_days': len(backtest_data),\n",
    "        'ticker_evaluated': test_ticker,\n",
    "        'models_evaluated': len(all_models)\n",
    "    },\n",
    "    'model_performance': model_performance,\n",
    "    'trading_performance': {}\n",
    "}\n",
    "\n",
    "# Add trading performance (without pandas objects)\n",
    "for model_name, perf in trading_performance.items():\n",
    "    report_data['trading_performance'][model_name] = {\n",
    "        k: v for k, v in perf.items() if k != 'portfolio_data'\n",
    "    }\n",
    "\n",
    "# Save comprehensive report\n",
    "report_path = get_project_path(\"experiments/results/final_evaluation_report.json\")\n",
    "save_results_to_json(report_data, report_path)\n",
    "\n",
    "print(\"ðŸ“‹ Final Performance Report\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Evaluation Date: {report_data['evaluation_metadata']['evaluation_date']}\")\n",
    "print(f\"Backtest Period: {report_data['evaluation_metadata']['backtest_period_start']} to {report_data['evaluation_metadata']['backtest_period_end']}\")\n",
    "print(f\"Models Evaluated: {report_data['evaluation_metadata']['models_evaluated']}\")\n",
    "\n",
    "if model_performance:\n",
    "    print(\"\\nðŸ† Top 3 Models by RÂ² Score:\")\n",
    "    for i, model in enumerate(performance_df_sorted.head(3).index, 1):\n",
    "        metrics = model_performance[model]\n",
    "        print(f\"  {i}. {model}\")\n",
    "        print(f\"     RÂ²: {metrics['r2']:.4f} | RMSE: {metrics['rmse']:.4f} | MAE: {metrics['mae']:.4f}\")\n",
    "\n",
    "if trading_performance:\n",
    "    print(\"\\nðŸ’° Trading Performance Summary:\")\n",
    "    for model in top_models:\n",
    "        if model in trading_performance:\n",
    "            perf = trading_performance[model]\n",
    "            print(f\"  {model}:\")\n",
    "            print(f\"    Total Return: {perf['total_return_strategy']:.2f}% vs Buy&Hold: {perf['total_return_buy_hold']:.2f}%\")\n",
    "            print(f\"    Excess Return: {perf['excess_return']:.2f}%\")\n",
    "            print(f\"    Sharpe Ratio: {perf['sharpe_ratio_strategy']:.3f}\")\n",
    "            print(f\"    Max Drawdown: {perf['max_drawdown_strategy']:.2f}%\")\n",
    "            print(f\"    Win Rate: {perf['win_rate']:.1f}%\")\n",
    "\n",
    "print(f\"\\nðŸ’¾ Comprehensive report saved to: {report_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e031a20",
   "metadata": {},
   "source": [
    "### ðŸŽ¯ Model Selection Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8ff00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸŽ¯ Model Selection Recommendations\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if model_performance and len(model_performance) > 0:\n",
    "    best_overall = performance_df_sorted.index[0]\n",
    "    best_metrics = model_performance[best_overall]\n",
    "    \n",
    "    print(f\"\\nðŸ¥‡ BEST OVERALL MODEL: {best_overall}\")\n",
    "    print(f\"   Prediction Accuracy (RÂ²): {best_metrics['r2']:.4f}\")\n",
    "    print(f\"   Error Metrics - RMSE: {best_metrics['rmse']:.4f}, MAE: {best_metrics['mae']:.4f}\")\n",
    "    print(f\"   MAPE: {best_metrics['mape']:.2f}%\")\n",
    "    \n",
    "    if best_overall in trading_performance:\n",
    "        trading_metrics = trading_performance[best_overall]\n",
    "        print(f\"   Trading Performance:\")\n",
    "        print(f\"     - Strategy Return: {trading_metrics['total_return_strategy']:.2f}%\")\n",
    "        print(f\"     - Excess over Buy&Hold: {trading_metrics['excess_return']:.2f}%\")\n",
    "        print(f\"     - Risk-Adjusted Return (Sharpe): {trading_metrics['sharpe_ratio_strategy']:.3f}\")\n",
    "    \n",
    "    # Model-specific recommendations\n",
    "    print(f\"\\nðŸ“Š DETAILED ANALYSIS:\")\n",
    "    \n",
    "    # Identify best performers in different categories\n",
    "    best_accuracy = performance_df_sorted.index[0]  # Highest RÂ²\n",
    "    best_precision = performance_df.sort_values('rmse').index[0]  # Lowest RMSE\n",
    "    \n",
    "    print(f\"   ðŸŽ¯ Most Accurate Predictions: {best_accuracy}\")\n",
    "    print(f\"   ðŸ” Most Precise (Low RMSE): {best_precision}\")\n",
    "    \n",
    "    if trading_performance:\n",
    "        # Find best trading performer\n",
    "        trading_returns = {model: perf['total_return_strategy'] \n",
    "                          for model, perf in trading_performance.items()}\n",
    "        best_trading = max(trading_returns, key=trading_returns.get)\n",
    "        \n",
    "        best_sharpe = max(trading_performance, \n",
    "                         key=lambda x: trading_performance[x]['sharpe_ratio_strategy'])\n",
    "        \n",
    "        print(f\"   ðŸ’° Best Trading Returns: {best_trading} ({trading_returns[best_trading]:.2f}%)\")\n",
    "        print(f\"   âš–ï¸ Best Risk-Adjusted: {best_sharpe}\")\n",
    "    \n",
    "    # Usage recommendations\n",
    "    print(f\"\\nðŸ“‹ USAGE RECOMMENDATIONS:\")\n",
    "    print(f\"   â€¢ For Maximum Accuracy: Use {best_overall}\")\n",
    "    print(f\"   â€¢ For Live Trading: Consider risk management with {best_overall}\")\n",
    "    print(f\"   â€¢ For Research: Compare top 3 models for ensemble approaches\")\n",
    "    \n",
    "    # Model insights\n",
    "    baseline_models = ['naive_last_value', 'random_walk_drift']\n",
    "    ml_models = [m for m in model_performance.keys() if m not in baseline_models]\n",
    "    \n",
    "    if ml_models and baseline_models:\n",
    "        best_ml = max(ml_models, key=lambda x: model_performance[x]['r2'])\n",
    "        best_baseline = max([m for m in baseline_models if m in model_performance], \n",
    "                           key=lambda x: model_performance[x]['r2'])\n",
    "        \n",
    "        ml_r2 = model_performance[best_ml]['r2']\n",
    "        baseline_r2 = model_performance[best_baseline]['r2']\n",
    "        improvement = ((ml_r2 - baseline_r2) / abs(baseline_r2)) * 100\n",
    "        \n",
    "        print(f\"\\nðŸ”¬ ML vs BASELINE COMPARISON:\")\n",
    "        print(f\"   Best ML Model: {best_ml} (RÂ²: {ml_r2:.4f})\")\n",
    "        print(f\"   Best Baseline: {best_baseline} (RÂ²: {baseline_r2:.4f})\")\n",
    "        print(f\"   ML Improvement: {improvement:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ee7f1a",
   "metadata": {},
   "source": [
    "### âš ï¸ Risk Assessment and Limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d278719",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"âš ï¸ Risk Assessment and Limitations\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Calculate volatility and risk metrics\n",
    "if backtest_results:\n",
    "    print(\"\\nðŸ“Š RISK ANALYSIS:\")\n",
    "    \n",
    "    for model_name in top_models[:2]:  # Top 2 models\n",
    "        results = backtest_results[model_name]\n",
    "        \n",
    "        # Calculate prediction volatility\n",
    "        pred_returns = results['predicted'].pct_change().dropna()\n",
    "        actual_returns = results['actual'].pct_change().dropna()\n",
    "        \n",
    "        pred_volatility = pred_returns.std() * np.sqrt(252) * 100  # Annualized\n",
    "        actual_volatility = actual_returns.std() * np.sqrt(252) * 100\n",
    "        \n",
    "        # Calculate prediction errors\n",
    "        errors = results['actual'] - results['predicted']\n",
    "        error_volatility = errors.std()\n",
    "        max_error = abs(errors).max()\n",
    "        \n",
    "        print(f\"\\n  {model_name}:\")\n",
    "        print(f\"    Prediction Volatility: {pred_volatility:.2f}% (Actual: {actual_volatility:.2f}%)\")\n",
    "        print(f\"    Maximum Single Error: ${max_error:.2f}\")\n",
    "        print(f\"    Error Standard Deviation: ${error_volatility:.2f}\")\n",
    "        \n",
    "        # Trading risk metrics\n",
    "        if model_name in trading_performance:\n",
    "            trading_perf = trading_performance[model_name]\n",
    "            print(f\"    Maximum Drawdown: {trading_perf['max_drawdown_strategy']:.2f}%\")\n",
    "            print(f\"    Trade Success Rate: {trading_perf['win_rate']:.1f}%\")\n",
    "\n",
    "print(\"\\nâš ï¸ KEY LIMITATIONS:\")\n",
    "print(\"   1. Backtest Period: Limited to recent historical data\")\n",
    "print(\"   2. Market Conditions: Performance may vary in different market regimes\")\n",
    "print(\"   3. Transaction Costs: Real trading costs not included in analysis\")\n",
    "print(\"   4. Liquidity: Assumes perfect execution at predicted prices\")\n",
    "print(\"   5. Model Stability: Performance may degrade over time without retraining\")\n",
    "print(\"   6. Overfitting Risk: Models may not generalize to unseen market conditions\")\n",
    "\n",
    "print(\"\\nðŸ›¡ï¸ RISK MITIGATION STRATEGIES:\")\n",
    "print(\"   â€¢ Regular model retraining and validation\")\n",
    "print(\"   â€¢ Position sizing based on prediction confidence\")\n",
    "print(\"   â€¢ Stop-loss mechanisms for risk management\")\n",
    "print(\"   â€¢ Diversification across multiple models/strategies\")\n",
    "print(\"   â€¢ Real-time monitoring of model performance\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623bed08",
   "metadata": {},
   "source": [
    "### ðŸ“ˆ Advanced Analysis and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58bc356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error analysis by time periods and market conditions\n",
    "print(\"ðŸ“ˆ Advanced Analysis and Insights\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if backtest_results and len(backtest_results) > 0:\n",
    "    # Analyze performance in different market conditions\n",
    "    best_model = performance_df_sorted.index[0]\n",
    "    results = backtest_results[best_model]\n",
    "    \n",
    "    # Calculate market conditions\n",
    "    results_analysis = results.copy()\n",
    "    results_analysis['actual_return'] = results_analysis['actual'].pct_change()\n",
    "    results_analysis['volatility'] = results_analysis['actual_return'].rolling(20).std()\n",
    "    results_analysis['trend'] = results_analysis['actual'].rolling(20).mean()\n",
    "    results_analysis['error'] = abs(results_analysis['actual'] - results_analysis['predicted'])\n",
    "    results_analysis['error_pct'] = results_analysis['error'] / results_analysis['actual'] * 100\n",
    "    \n",
    "    # Market regime classification\n",
    "    volatility_threshold = results_analysis['volatility'].quantile(0.7)\n",
    "    results_analysis['market_regime'] = np.where(\n",
    "        results_analysis['volatility'] > volatility_threshold, 'High_Volatility', 'Low_Volatility'\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nðŸ” PERFORMANCE BY MARKET CONDITIONS ({best_model}):\")\n",
    "    \n",
    "    for regime in ['Low_Volatility', 'High_Volatility']:\n",
    "        regime_data = results_analysis[results_analysis['market_regime'] == regime]\n",
    "        if len(regime_data) > 10:\n",
    "            avg_error = regime_data['error'].mean()\n",
    "            avg_error_pct = regime_data['error_pct'].mean()\n",
    "            r2_regime = calculate_r2(regime_data['actual'].values, regime_data['predicted'].values)\n",
    "            \n",
    "            print(f\"  {regime.replace('_', ' ')} Markets:\")\n",
    "            print(f\"    Average Error: ${avg_error:.2f} ({avg_error_pct:.2f}%)\")\n",
    "            print(f\"    RÂ² Score: {r2_regime:.4f}\")\n",
    "            print(f\"    Data Points: {len(regime_data)}\")\n",
    "\n",
    "    # Time-based performance analysis\n",
    "    print(f\"\\nðŸ“… PERFORMANCE BY TIME PERIODS:\")\n",
    "    \n",
    "    # Monthly performance\n",
    "    results_analysis['month'] = results_analysis.index.month\n",
    "    monthly_performance = results_analysis.groupby('month').agg({\n",
    "        'error': 'mean',\n",
    "        'error_pct': 'mean'\n",
    "    })\n",
    "    \n",
    "    best_month = monthly_performance['error'].idxmin()\n",
    "    worst_month = monthly_performance['error'].idxmax()\n",
    "    \n",
    "    month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
    "                   'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "    \n",
    "    print(f\"    Best Performance Month: {month_names[best_month-1]} (Error: ${monthly_performance.loc[best_month, 'error']:.2f})\")\n",
    "    print(f\"    Worst Performance Month: {month_names[worst_month-1]} (Error: ${monthly_performance.loc[worst_month, 'error']:.2f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7da1a2",
   "metadata": {},
   "source": [
    "### ðŸ“Š Final Visualization Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f78d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive dashboard\n",
    "if backtest_results and model_performance:\n",
    "    fig = plt.figure(figsize=(20, 16))\n",
    "    fig.suptitle('ðŸ“Š Comprehensive Model Evaluation Dashboard', fontsize=20, fontweight='bold')\n",
    "    \n",
    "    # Create grid layout\n",
    "    gs = fig.add_gridspec(4, 4, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    # 1. Model Performance Comparison (2x2)\n",
    "    ax1 = fig.add_subplot(gs[0:2, 0:2])\n",
    "    \n",
    "    models = list(model_performance.keys())\n",
    "    r2_scores = [model_performance[m]['r2'] for m in models]\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(models)))\n",
    "    \n",
    "    bars = ax1.barh(models, r2_scores, color=colors)\n",
    "    ax1.set_xlabel('RÂ² Score')\n",
    "    ax1.set_title('Model Performance Ranking (RÂ² Score)', fontweight='bold', fontsize=14)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, value in zip(bars, r2_scores):\n",
    "        width = bar.get_width()\n",
    "        ax1.text(width + 0.01, bar.get_y() + bar.get_height()/2,\n",
    "                f'{value:.3f}', ha='left', va='center', fontweight='bold')\n",
    "    \n",
    "    # 2. Best Model Predictions (2x2)\n",
    "    ax2 = fig.add_subplot(gs[0:2, 2:4])\n",
    "    \n",
    "    best_model = performance_df_sorted.index[0]\n",
    "    best_results = backtest_results[best_model]\n",
    "    \n",
    "    # Plot last 60 days for clarity\n",
    "    recent_data = best_results.tail(60)\n",
    "    ax2.plot(recent_data.index, recent_data['actual'], 'b-', label='Actual', linewidth=2)\n",
    "    ax2.plot(recent_data.index, recent_data['predicted'], 'r--', label='Predicted', linewidth=2)\n",
    "    \n",
    "    ax2.set_title(f'Best Model Predictions: {best_model}', fontweight='bold', fontsize=14)\n",
    "    ax2.set_ylabel('Price ($)')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 3. Trading Performance (1x2)\n",
    "    if trading_performance:\n",
    "        ax3 = fig.add_subplot(gs[2, 0:2])\n",
    "        \n",
    "        trading_models = list(trading_performance.keys())\n",
    "        excess_returns = [trading_performance[m]['excess_return'] for m in trading_models]\n",
    "        \n",
    "        bars = ax3.bar(trading_models, excess_returns, \n",
    "                      color=['green' if x > 0 else 'red' for x in excess_returns])\n",
    "        ax3.set_title('Excess Returns vs Buy & Hold', fontweight='bold', fontsize=14)\n",
    "        ax3.set_ylabel('Excess Return (%)')\n",
    "        ax3.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "        ax3.tick_params(axis='x', rotation=45)\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, value in zip(bars, excess_returns):\n",
    "            height = bar.get_height()\n",
    "            ax3.text(bar.get_x() + bar.get_width()/2., height + (0.5 if height > 0 else -0.5),\n",
    "                    f'{value:.1f}%', ha='center', va='bottom' if height > 0 else 'top', \n",
    "                    fontweight='bold')\n",
    "    \n",
    "    # 4. Risk Metrics (1x2)\n",
    "    ax4 = fig.add_subplot(gs[2, 2:4])\n",
    "    \n",
    "    if trading_performance:\n",
    "        models_risk = list(trading_performance.keys())\n",
    "        drawdowns = [abs(trading_performance[m]['max_drawdown_strategy']) for m in models_risk]\n",
    "        sharpe_ratios = [trading_performance[m]['sharpe_ratio_strategy'] for m in models_risk]\n",
    "        \n",
    "        # Scatter plot: Drawdown vs Sharpe Ratio\n",
    "        scatter = ax4.scatter(drawdowns, sharpe_ratios, s=100, alpha=0.7, c=colors[:len(models_risk)])\n",
    "        ax4.set_xlabel('Maximum Drawdown (%)')\n",
    "        ax4.set_ylabel('Sharpe Ratio')\n",
    "        ax4.set_title('Risk vs Return Profile', fontweight='bold', fontsize=14)\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add model labels\n",
    "        for i, model in enumerate(models_risk):\n",
    "            ax4.annotate(model, (drawdowns[i], sharpe_ratios[i]), \n",
    "                        xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "    \n",
    "    # 5. Performance Summary Table (1x4)\n",
    "    ax5 = fig.add_subplot(gs[3, :])\n",
    "    ax5.axis('tight')\n",
    "    ax5.axis('off')\n",
    "    \n",
    "    # Create summary table\n",
    "    summary_data = []\n",
    "    for model in performance_df_sorted.head(5).index:\n",
    "        row = [\n",
    "            model,\n",
    "            f\"{model_performance[model]['r2']:.4f}\",\n",
    "            f\"{model_performance[model]['rmse']:.3f}\",\n",
    "            f\"{model_performance[model]['mae']:.3f}\",\n",
    "            f\"{model_performance[model]['mape']:.1f}%\"\n",
    "        ]\n",
    "        \n",
    "        if model in trading_performance:\n",
    "            row.extend([\n",
    "                f\"{trading_performance[model]['total_return_strategy']:.1f}%\",\n",
    "                f\"{trading_performance[model]['excess_return']:.1f}%\",\n",
    "                f\"{trading_performance[model]['sharpe_ratio_strategy']:.2f}\"\n",
    "            ])\n",
    "        else:\n",
    "            row.extend(['-', '-', '-'])\n",
    "        \n",
    "        summary_data.append(row)\n",
    "    \n",
    "    columns = ['Model', 'RÂ²', 'RMSE', 'MAE', 'MAPE', 'Strategy Return', 'Excess Return', 'Sharpe Ratio']\n",
    "    \n",
    "    table = ax5.table(cellText=summary_data, colLabels=columns, cellLoc='center', loc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1.2, 1.5)\n",
    "    \n",
    "    # Style the table\n",
    "    for i in range(len(columns)):\n",
    "        table[(0, i)].set_facecolor('#4CAF50')\n",
    "        table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "    \n",
    "    # Highlight best model row\n",
    "    for i in range(len(columns)):\n",
    "        table[(1, i)].set_facecolor('#E8F5E8')\n",
    "    \n",
    "    ax5.set_title('Performance Summary - Top 5 Models', fontweight='bold', fontsize=14, pad=20)\n",
    "    \n",
    "    plt.savefig(get_project_path(\"experiments/figures/comprehensive_evaluation_dashboard.png\"), \n",
    "                dpi=300, bbox_inches='tight', facecolor='white')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768ca80f",
   "metadata": {},
   "source": [
    "### ðŸ”š Final Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb416921",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ”š Final Summary and Next Steps\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if model_performance:\n",
    "    total_models = len(model_performance)\n",
    "    successful_models = len([m for m in model_performance.values() if m['r2'] > 0])\n",
    "    \n",
    "    print(f\"âœ… EVALUATION COMPLETED SUCCESSFULLY\")\n",
    "    print(f\"   â€¢ Total Models Evaluated: {total_models}\")\n",
    "    print(f\"   â€¢ Models with Positive RÂ²: {successful_models}\")\n",
    "    print(f\"   â€¢ Backtest Period: {len(backtest_data)} days\")\n",
    "    \n",
    "    if trading_performance:\n",
    "        profitable_strategies = len([m for m in trading_performance.values() if m['total_return_strategy'] > 0])\n",
    "        print(f\"   â€¢ Profitable Trading Strategies: {profitable_strategies}/{len(trading_performance)}\")\n",
    "    \n",
    "    print(f\"\\nðŸ† CHAMPION MODEL: {performance_df_sorted.index[0]}\")\n",
    "    champion_metrics = model_performance[performance_df_sorted.index[0]]\n",
    "    print(f\"   â€¢ Prediction Accuracy (RÂ²): {champion_metrics['r2']:.4f}\")\n",
    "    print(f\"   â€¢ Mean Absolute Error: ${champion_metrics['mae']:.2f}\")\n",
    "    \n",
    "    if performance_df_sorted.index[0] in trading_performance:\n",
    "        champion_trading = trading_performance[performance_df_sorted.index[0]]\n",
    "        print(f\"   â€¢ Trading Return: {champion_trading['total_return_strategy']:.2f}%\")\n",
    "        print(f\"   â€¢ Risk-Adjusted Return: {champion_trading['sharpe_ratio_strategy']:.3f}\")\n",
    "\n",
    "print(f\"\\nðŸ“ ALL RESULTS SAVED TO:\")\n",
    "print(f\"   â€¢ Models: {models_dir}\")\n",
    "print(f\"   â€¢ Results: {results_dir}\")\n",
    "print(f\"   â€¢ Figures: {figures_dir}\")\n",
    "\n",
    "print(f\"\\nðŸš€ RECOMMENDED NEXT STEPS:\")\n",
    "print(f\"   1. Deploy champion model for live predictions\")\n",
    "print(f\"   2. Set up automated retraining pipeline\")\n",
    "print(f\"   3. Implement risk management system\")\n",
    "print(f\"   4. Monitor model performance in production\")\n",
    "print(f\"   5. Collect feedback for model improvements\")\n",
    "\n",
    "print(f\"\\nðŸ“Š FOR PRODUCTION DEPLOYMENT:\")\n",
    "print(f\"   â€¢ Use: {performance_df_sorted.index[0]} model\")\n",
    "print(f\"   â€¢ Monitor: Prediction accuracy and trading performance\")\n",
    "print(f\"   â€¢ Retrain: Monthly or when performance degrades\")\n",
    "print(f\"   â€¢ Risk: Implement position sizing and stop-losses\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ EVALUATION COMPLETE!\")\n",
    "print(\"=\" * 60)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stock-predictor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
